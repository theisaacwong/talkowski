---
title: "gCNV_pipeline_2020_07_06"
output: html_document
author: Isaac Wong
version: 2.15
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This workflow is an attempt to provide instructions on running gCNV. Please let me know of any isses you run into or any processes you want improved/automated. Please ensure you have downloaded the most recent files from my github: https://github.com/theisaacwong/talkowski/tree/master/gCNV. You should be able to run the pipeline chunk by chunk.Basic GATK is below:
```{}
        gatk --java-options "-Xmx${command_mem_mb}m" CollectReadCounts \
            -I ${cram} \
            --read-index ${crai} \
            -L ${intervals_barcode} \
            --interval-merging-rule OVERLAPPING_ONLY \
            --reference ${hg38_reference} \
            --format TSV \
            -O ${counts_barcode_filename}
```


#Step 0
Call GATK's CollectReadcounts on bam files to get interval coverage. (https://gatk.broadinstitute.org/hc/en-us/articles/360036729331-CollectReadCounts).Bsst practice is to batch samples into groups of ~400 and run the WDL on each batch separately. 

Here is a WDL to run it: https://portal.firecloud.org/?return=terra#methods/countingMethods/countingCrams/8/wdl


To begin, load the required packages and some functions
```{r}
library("stringr")
library("plyr")
library("rgl")
library("factoextra")
library("tidyverse")
library("clusterSim")
library("cluster")
library("clValid")
library("GenomicRanges")
library("parallel")
library("gridExtra")
library("wesanderson")
library("ggplot2")
library("wesanderson")
library("factoextra")
library("ClusterR")
library("dbscan")


srt <- function(x){sort(table(x), decreasing = TRUE)}

makeFiles <- function(name, cohort, mat, gbucket){
    output_name <- paste0(cohort, "-", name, ".txt")
    colind <- which(colnames(mat)==name)
    tmp <- mat[,colind]
    tmp <- str_replace_all(tmp, '\"', "")
    files <- unlist(str_split(str_replace_all(tmp, "(\\[)|]", ""), ","))
    write.table(files, quote=FALSE, sep="\t", col.names=F, row.names=F, file=paste0("~/downloads/", output_name))
    system2("gsutil", paste0("cp ~/downloads/", output_name, " ", gbucket, output_name))
    #file.remove(paste0("~/downloads/", output_name))
}

wrapper <- function(x, ...) {
  paste(strwrap(x, ...), collapse = "\n")
}

plot_ploidy <- function(df, i=1){
  ggplot(df, aes(x=START, y=LINEAR_COPY_RATIO)) + 
          geom_point(alpha=0.2) + 
          geom_hline(aes(yintercept=2)) +
          geom_hline(aes(yintercept=1)) +
          ylim(-0.5, 4) + 
          labs(x="Start Coordinate", 
               y="linear copy ratio", 
               title=sprintf("%s; %s; ploidy: %s", 
                             df$sample[1], 
                             df$CONTIG[i], 
                             ploidy_df[ploidy_df$CONTIG==df$CONTIG[i] & ploidy_df$sample==df$sample[1],]$PLOIDY[1]) %>% wrapper(30) )
}
```

#Step 1
Manally download the sample_set_entity.tsv file from firecloud. Set the wd (working directory) and jar_path variables to point to the working directory and the gCNV_helper.jar file. Best practices for directory structure:
  
  - Each time you download the zip file containing sample_set_entity.tsv and sample_set_membership.tsv, extract the zip file into a labeled directory, then change the wd to that directory. This will help with version control, file naming scheme, and pipeline reproducabiliy. In the eventuallity that you need to redo a step or trace previous steps, this is very helpful. 
  -You wil eventuly need to download the manifest file mulitple times and will end up having different manifest files with the same name. Here is what my project directory looks like
        gCNV_project_name/
        |-- sample_set_entity_barcode_2020_01_01/
        |   |-- countCramGrp_1/
        |   |   |-- sample_0001.barcode.counts.tsv
        |   |   |-- sample_0002.barcode.counts.tsv ...
        |   |-- countCramGrp_2/
        |   |   |-- sample_0501.barcode.counts.tsv
        |   |   |-- sample_0502.barcode.counts.tsv ...
        |   |-- countCramGrp_3/ ...
        |   |-- sample_set_entity.tsv
        |   |-- sample_set_membership.tsv
        |   |-- pca.rda
        |   |-- counts_matrix.tsv
        |
        |-- sample_set_entity_dataMunging_2020_01_07/
        |   |-- sample_set_entity.tsv
        |   |-- sample_set_membership.tsv
        |   |-- new_labels.tsv
        |
        |-- sample_set_entity_clustering_2020_01_14/
        |   |-- cluster_1_1_CASE/
        |   |   |-- cluster_1_1_CASE.java.bed
        |   |   |-- genotyped-segments-sample_0001.vcf
        |   |   |-- genotyped-segments-sample_0001.vcf.gz
        |   |   |-- genotyped-segments-sample_0002.vcf
        |   |   |-- genotyped-segments-sample_0002.vcf.gz ...
        |   |-- cluster_1_1_COHORT/
        |   |   |-- cluster_1_1_COHORT.java.bed
        |   |   |-- genotyped-segments-sample_0501.vcf
        |   |   |-- genotyped-segments-sample_0501.vcf.gz
        |   |   |-- genotyped-segments-sample_0502.vcf
        |   |   |-- genotyped-segments-sample_0502.vcf.gz ...
        |   |-- cluster_1_2_CASE/ ...
        |   |-- sample_set_entity.tsv
        |   |-- sample_set_membership.tsv
        |   |-- clustered.bed
        |   |-- gcnv_defragged.tsv
        |   |-- svtk_input.tsv
        |   |-- svtk_output.tsv
        |   |-- out.rda
        |   |-- final_callset_2020_01_14.tsv
        |
        |-- Plots
            |-- QC_plot_01.pdf
            |-- QC_plot_02.pdf
            |-- QC_plot_03.pdf
        
```{r}
wd <- "/path/to/folder/"
jar_path <- "/path/to/jar"
system2("java", sprintf("-Xmx16G -jar %s --help", jar_path)) # you might need to change -Xmx to how much memory you have available on your device
```


#Step 2
Download the barcode counts files to the wd. If the column name in your manifest file isn't "counts_barcode" then change code. This chunk just makes a system call to run the jar, which itself makes a system call to gsutil to download the files. It's turtles all the way down. 
```{r}
wd <- "/path/to/folder/"
system2("java", sprintf("-Xmx16G -jar %s getBarcodeCounts %ssample_set_entity.tsv %s counts_barcode", jar_path, wd, wd ))
```

#Step 3
Convert the barcode count files into a matrix file where rows are samples and columns are intervals
```{r}
wd <- "/path/to/folder/"
system2("java", sprintf("-Xmx16G -jar %s getCountsMatrix %s %scounts_matrix.tsv .barcode.counts.tsv", jar_path, wd, wd ))
```

#Step 4
- perform data normalization
- remove x and y chromosomes
- perform PCA
- save the R PCA object
a scree plot for the PCA loadings is generated to help visualize the data. 
```{r}
wd <- ""
counts_df <- read.table(paste0(wd, "counts_matrix.tsv"), sep="\t", stringsAsFactors = FALSE, header=TRUE, fill=TRUE)

counts_df[is.na(counts_df)] <- 0
counts_matrix <- t(as.matrix(counts_df))
indexes_to_remove <- rep(FALSE, nrow(counts_matrix))
rown <- rownames(counts_matrix)
chr <- do.call(rbind, str_split(rown, "_"))[,1]
bool_y <- chr == "X" | chr == "chrX" | chr == "ChrX"
bool_x <- chr == "Y" | chr == "chrY" | chr == "ChrY"
indexes_to_remove[bool_y | bool_x] <- TRUE
mydata_filtered <- counts_matrix[!indexes_to_remove,]
mydataNormalized <- t(t(mydata_filtered)/colSums(mydata_filtered, na.rm = TRUE))

pca <- prcomp(t(mydataNormalized), rank = 7)
fviz_eig(pca)
plot3d(pca$x[,c(1,2,3)])
save(pca, file=paste0(wd, "pca.rda"))
x <- pca$x
save(x, file=paste0(wd, "x.rda"))
```


```{r}
open3d()
rgl.bg(color = "grey") 
plot3d(pca$x[,1:3], col=colors)
if (!rgl.useNULL())
  play3d(spin3d(axis = c(1, 1, 1), rpm = 4), duration = 30, )
```


#Step 5
- Pseudo-automate clustering. 
This step uses unsupervised clustering and metrics, so manual curration of final clustering is highly recomended. Typically, the best choice is among the top four recommended options. I am currently using only hclust. From previous experiences, it tends to work the best on average. Density based methods are currently in development. The rval data frame will store the results of each clustering metric. The final column is a ranking of all the metrics. By default, the first choice is chosen for clustering. However, you should manually check the data frame to see if any other combination has similar rankings in metrics to the top choice. The top choice is not necessarily the best. Change the 'n_clusters' variable based on the number of clusters you want. 
```{r}
wd <- ""
load(paste0(wd, "x.rda"))
load(paste0(wd, "pca.rda"))

set.seed(123)
pca_loadings <- x
n_clusters <- 15
hclust_methods <- c("ward.D", "ward.D2", "single", "complete", "average", "mcquitty", "median", "centroid") 
distance_methods <- c("euclidean", "maximum", "canberra", "minkowski")
mink_p <- c(10)
results <- expand.grid(hclust_methods, distance_methods)
colnames(results) <- c("agglomeration", "distance")
results$db <- 0
results$silhouette <- 0
results$dunn <- 0

dist_eucl <- dist(pca_loadings[,1:3], method="euclidean") 
dist_maxi <- dist(pca_loadings[,1:3], method="maximum") 
dist_canb <- dist(pca_loadings[,1:3], method="canberra") 
dist_mink <- dist(pca_loadings[,1:3], method="minkowski", p=2)

cuts <- vector("list", n_clusters)

USE_METRICS <- FALSE

for(i in 1:nrow(results)){
  print(i)
  dist_mat <- switch(results$distance[i] %>% as.character(), "euclidean"=dist_eucl, "maximum"=dist_maxi, "canberra"=dist_canb, "minkowski"=dist_mink)
  hclust <- hclust(dist_mat, method=results$agglomeration[i]) 
  cut_avg <- cutree(hclust, k=n_clusters)
  temp_names_srt <- names(srt(cut_avg))
  cuts[[i]] <- sapply(unname(cut_avg), function(x) which(x == temp_names_srt))
  
  if(USE_METRICS){
    results$db[i] <- index.DB(pca_loadings[,1:3], cut_avg, centrotypes="centroids", p=3)$DB
    results$silhouette[i] <- mean(silhouette(cut_avg, dist_mat)[,3])
    results$dunn[i] <- dunn(dist_mat, cut_avg)    
  }
}

if(USE_METRICS){
  results <- results[order(results$silhouette, decreasing = TRUE),]
  rval <- results
  rval$db <- order(results$db, decreasing = FALSE)
  rval$dunn <- order(results$dunn, decreasing = TRUE)
  rval$silhouette <- order(results$silhouette, decreasing = TRUE)
  rval$sum <- rval$db + rval$dunn + rval$silhouette
  rval <- rval[order(rval$sum),]
  write.table(rval , paste0(wd, "clustering_metrics", n_clusters, ".tsv"), sep="\t", quote = FALSE, col.names = TRUE, row.names = FALSE)
}

ct <- as.data.frame(do.call(rbind, lapply(cuts, table)))
ct <- ct[order(ct[,1]), ]
```

the top row of ct is often a good choice as well

#Step 6
Plot the clustering output. Change the 'choice' variable from 1 to 33 to cycle through the different clustering methods. (lower is "better")
```{r}
n <- n_clusters
choice <- 10
sorted_clusters <- cuts[[choice]]
cols <- rainbow(n)[sample(1:n, n)]
plot3d(x[,1:3], col=cols[sorted_clusters])
```


merge the small clusters to their nearest neighbor
```{r}
set.seed(123)
threshold <- 150
cluster_centers <- do.call(rbind, lapply(1:n_clusters, function(i){
  message(i)
  if(length(x[sorted_clusters==i, ]) == 7){
    return(x[sorted_clusters==i, ])    
  } else {
    colMeans(x[sorted_clusters==i, ])  
  }
}) ) %>% as.data.frame()

tab <- table(sorted_clusters)
which_clusters_to_relabel <- which(tab < threshold) %>% unname
clusters_to_relabel <- x[sorted_clusters %in% which_clusters_to_relabel, ]
bool_cluster_relabel <- sorted_clusters %in% which_clusters_to_relabel
main_cluster_centers <- cluster_centers[-c(which_clusters_to_relabel),]


nearest_clusters <- lapply(1:nrow(x), function(i){
  if(!bool_cluster_relabel[i]){ 
    return(sorted_clusters[i])
  } else {
    dists <- as.matrix(dist(rbind(x[i, ], main_cluster_centers)))[1, -1]
    return(as.numeric(names(dists)[which(dists==min(dists))][1])-1)
  }
}) %>% unlist %>% as.numeric()

l1 <- length(unique(nearest_clusters))
cols <- rainbow(l1)[sample(1:l1, l1)]
plot3d(x[,1:3], col=cols[nearest_clusters])


cohort_size <- 200
memb <- do.call(rbind, lapply(sort(unique(nearest_clusters)), function(i){
  if(length(which(sorted_clusters==i)) <= cohort_size){
    df <- data.frame(membership=paste0("cluster_22k_", i, "_COHORT"), 
                     samples = row.names(pca$x)[which(nearest_clusters==i)])
    return(df)
  } else {
    cohort_subset_indexes <- base::sample(which(sorted_clusters==i), cohort_size, replace = FALSE)
    nearest_points_in_cluster_indexes <- which(nearest_clusters==i)
    case_subset_indexes <- base::setdiff(nearest_points_in_cluster_indexes, cohort_subset_indexes)
    
    cohort_label <- paste0("cluster_22k_", i, "_COHORT")
    case_label <- paste0("cluster_22k_", i, "_CASE")
    
    cohort_samples <- row.names(pca$x)[cohort_subset_indexes]
    case_samples <- row.names(pca$x)[case_subset_indexes]
    
    df <- data.frame(membership=c(
      rep(cohort_label, length(cohort_samples)), 
      rep(case_label, length(case_samples)) ), 
      samples=c(
        cohort_samples,
        case_samples),
      stringsAsFactors = FALSE)
    return(df)
  }
}) )
write.table(memb, paste0(wd, "membership.tsv"), sep="\t", col.names = TRUE, row.names = FALSE, quote=FALSE)
```

```{r}
cols <- wes_palette("Darjeeling1")
cols <- rainbow(n)[sample(1:n, n)]
open3d()
rgl.bg(color = "white") 
plot3d(x[,1:3], col=cols[nearest_clusters])
if (!rgl.useNULL())
  play3d(spin3d(axis = c(1, 1, 1), rpm = 4), duration = 30, )
```

# cluster on sex chromosome
```{r}
files <- list.files(wd, pattern=".exons.counts.tsv", full.names = TRUE, recursive = TRUE)

cluster <- makeCluster(detectCores()-1)
clusterExport(cl=cluster, varlist=c("files", "str_replace_all"))
counts_dfs <- parLapply(cluster, files, function(x){
   cbind(read.table(x, sep="\t", header=TRUE, stringsAsFactors = FALSE, col.names = c("chr", "start", "end", "count"), comment.char = "@"),
        str_replace_all(basename(x), ".exons.counts.tsv", ""))
})
stopCluster(cluster)


names(counts_dfs) <- str_replace_all(basename(files), ".exons.counts.tsv", "")

total_counts <- lapply(counts_dfs, function(x){
  x <- x[!(x$start<=2781479 & x$end>=10001),] # remove PAR1
  x <- x[!((x$chr=="chrX" & x$start<=156030895 & x$end>=155701383) | 
           (x$chr=="chrY" & x$start<=57217415 & x$end>=56887903)),] # remove PAR2
  data.frame(xcount = sum(x$count[x$chr=="chrX"]), 
             ycount = sum(x$count[x$chr=="chrY"]), 
             sample = as.character(x[1,5]), stringsAsFactors = FALSE)
})

merged_counts <- do.call(rbind, total_counts)
merged_counts$x <- merged_counts$xcount / median(merged_counts$xcount)
merged_counts$y <- merged_counts$ycount / median(merged_counts$ycount)

memb <- read.table(paste0(wd, "membership.tsv"), sep="\t", header=TRUE, stringsAsFactors = FALSE)
memb <- memb[str_detect(memb$membership.sample_set_id, "cluster.*[COHORT_CASE]"), ]

merged_counts$membership <- sapply(merged_counts$sample, function(x){
  unique(memb$membership.sample_set_id[grepl(x, memb$sample)])[1]
})
merged_counts$super_cluster <- merged_counts$membership %>% str_replace_all("_(COHORT|CASE)", "")

ggplot(merged_counts, aes(x=x, y=y, color=super_cluster)) + 
  geom_point() + 
  xlab("normalized chrX counts") + 
  ylab("normalized chrY counts") + 
  ggtitle("normalized chrX and chrY counts")
```

The X/Y counts form an elipsis shape (generally) with usually a handful of outliers. GMM has the best performance so far in for this unsupervised clustering process. Plots the cluster labels after for manual checking. 
```{r}
set.seed(4)

n_super_clusters <- merged_counts$super_cluster %>% unique %>% length
super_clusters <- merged_counts$super_cluster %>% unique
merged_counts$sex_cluster <- "-1"
merged_counts$sex_cluster_sub <- "-1"

for(i in 1:n_super_clusters){
  n_sub_cluster <- 2
  current_super_cluster <- merged_counts[merged_counts$super_cluster == super_clusters[i],]

  gclust <- GMM(current_super_cluster[, c("x", "y")], n_sub_cluster, km_iter = 10, em_iter=50)
  pr <- predict_GMM(current_super_cluster[, c("x", "y")], gclust$centroids, gclust$covariance_matrices, gclust$weights)
 # plot(current_super_cluster[, c("x", "y")], col=c("RED","BLUE")[pr$cluster_labels+1], pch=20, main=super_clusters[i])
  labels <- c("A", "B")[1 + pr$cluster_labels]
  sex_labels <- labels
  
  # this is quite bad rn, B, C denote quality
  x_mean_A <- mean(current_super_cluster$x[labels=="A"])
  x_mean_B <- mean(current_super_cluster$x[labels=="B"])
  y_mean_A <- mean(current_super_cluster$y[labels=="A"])
  y_mean_B <- mean(current_super_cluster$y[labels=="B"])
  if(x_mean_A < x_mean_B & y_mean_A > y_mean_B){
    sex_labels[sex_labels=="A"] <- "Y"
    sex_labels[sex_labels=="B"] <- "X"
  } else if(x_mean_A > x_mean_B & y_mean_A < y_mean_B) {
    sex_labels[sex_labels=="A"] <- "X"
    sex_labels[sex_labels=="B"] <- "Y"
  } else if(y_mean_A > y_mean_B){
    sex_labels[sex_labels=="A"] <- "YB"
    sex_labels[sex_labels=="B"] <- "XB"
  } else if(y_mean_A < y_mean_B) {
    sex_labels[sex_labels=="A"] <- "XB"
    sex_labels[sex_labels=="B"] <- "YB"
  } else if(x_mean_A < x_mean_B){
    sex_labels[sex_labels=="A"] <- "YC"
    sex_labels[sex_labels=="B"] <- "XC"
  } else if(x_mean_A > x_mean_B) {
    sex_labels[sex_labels=="A"] <- "XC"
    sex_labels[sex_labels=="B"] <- "YC"
  } 
  sub_labels <- sex_labels
  labels <- sub_labels
  current_super_cluster$sub_labels <- sex_labels
  print(
  #g1 <-  
    ggplot(current_super_cluster, aes(x=x, y=y, color=sub_labels)) + 
    geom_point() + 
    xlab("normalized chrX counts") + 
    ylab("normalized chrY counts") + 
    ggtitle(super_clusters[i]) 
  )
  
  # ggsave(filename=paste0("C:/Users/iwong/Documents/MGH/IBD/more_samples/figrues/", super_clusters[i], "_chr_X_&_Y_counts_GMM.png"), 
  #        plot=g1, 
  #        device=png()
  # )
  
  merged_counts$sex_cluster[merged_counts$super_cluster == super_clusters[i]] <- paste0(super_clusters[i], "_", labels) 
  
  for(k in unique(labels)){
    if(length(which(labels==k)) <= 200){
      sub_labels[which(labels==k)] <- paste0(sub_labels[which(labels==k)], "_COHORT")
    } else {
      cohort_subset_indexes <- base::sample(which(labels==k), 200, replace = FALSE)
      case_subset_indexes <- base::setdiff(which(labels==k), cohort_subset_indexes)
      
      sub_labels[cohort_subset_indexes] <- paste0(sub_labels[cohort_subset_indexes], "_COHORT")
      sub_labels[case_subset_indexes] <- paste0(sub_labels[case_subset_indexes], "_CASE")
      
    }
  }
  
  merged_counts$sex_cluster_sub[merged_counts$super_cluster == super_clusters[i]] <- paste0(super_clusters[i], "_", sub_labels) 
}
table(merged_counts$sex_cluster_sub)
memb <- data.frame(membership.sample_set_id=merged_counts$sex_cluster_sub, sample=merged_counts$sample)
write.table(memb, "membership.tsv", sep="\t", col.names = TRUE, row.names = FALSE, quote=FALSE)
```

DBSCAN is the best for when GMM doesn't work. fill in the vector super_clusters_to_re_cluster with the clusters that didn't work. DBSCAN isn't used as the first algorithm because it doesn't handle outliers as well
```{r}
set.seed(123)
super_clusters_to_re_cluster <- c("")

for(i in super_clusters_to_re_cluster){
  current_super_cluster <- merged_counts[merged_counts$super_cluster == i,]
  
  dclust <- dbscan(current_super_cluster[, c("x", "y")], eps=0.1, minPts = 10); table(dclust$cluster); lab <- dclust$cluster + 1
  n_clusters <- length(unique(lab))
  x <- current_super_cluster[, c("x", "y")]
  sorted_clusters <- sapply(unname(lab), function(x) which(x == names(srt(lab))))
  cluster_centers <- do.call(rbind, lapply(1:n_clusters, function(i){
    if(nrow(x[sorted_clusters==i, ]) == 1){
      return(x[sorted_clusters==i, ])    
    } else {
      colMeans(x[sorted_clusters==i, ])  
    }
  }) ) %>% as.data.frame()
  
  tab <- table(sorted_clusters)
  which_clusters_to_relabel <- sorted_clusters[!sorted_clusters %in% c(1,2)] %>% unique()
  bool_cluster_relabel <- sorted_clusters %in% which_clusters_to_relabel
  main_cluster_centers <- cluster_centers[-c(which_clusters_to_relabel),]
  
  nearest_clusters <- lapply(1:nrow(x), function(i){
    if(!bool_cluster_relabel[i]){ 
      return(sorted_clusters[i])
    } else {
      dists <- as.matrix(dist(rbind(x[i, ], main_cluster_centers)))[1, -1]
      return(as.numeric(names(dists)[which(dists==min(dists))][1]))
    }
  }) %>% unlist %>% as.numeric()
  
  labels <- c("A", "B")[nearest_clusters]
  sex_labels <- labels
  
  # this is quite bad rn, B, C denote quality
  x_mean_A <- mean(current_super_cluster$x[labels=="A"])
  x_mean_B <- mean(current_super_cluster$x[labels=="B"])
  y_mean_A <- mean(current_super_cluster$y[labels=="A"])
  y_mean_B <- mean(current_super_cluster$y[labels=="B"])
  if(x_mean_A < x_mean_B & y_mean_A > y_mean_B){
    sex_labels[sex_labels=="A"] <- "Y"
    sex_labels[sex_labels=="B"] <- "X"
  } else if(x_mean_A > x_mean_B & y_mean_A < y_mean_B) {
    sex_labels[sex_labels=="A"] <- "X"
    sex_labels[sex_labels=="B"] <- "Y"
  } else if(y_mean_A > y_mean_B){
    sex_labels[sex_labels=="A"] <- "YB"
    sex_labels[sex_labels=="B"] <- "XB"
  } else if(y_mean_A < y_mean_B) {
    sex_labels[sex_labels=="A"] <- "XB"
    sex_labels[sex_labels=="B"] <- "YB"
  } else if(x_mean_A < x_mean_B){
    sex_labels[sex_labels=="A"] <- "YC"
    sex_labels[sex_labels=="B"] <- "XC"
  } else if(x_mean_A > x_mean_B) {
    sex_labels[sex_labels=="A"] <- "XC"
    sex_labels[sex_labels=="B"] <- "YC"
  } 
  sub_labels <- sex_labels
  labels <- sub_labels
  current_super_cluster$sub_labels <- sex_labels
  print(ggplot(current_super_cluster, aes(x=x, y=y, color=sub_labels)) + 
  geom_point() + 
  xlab("normalized chrX counts") + 
  ylab("normalized chrY counts") + 
  ggtitle(i) )
  
  merged_counts$sex_cluster[merged_counts$super_cluster == i] <- paste0(i, "_", labels) 
  
  for(k in unique(labels)){
    if(length(which(labels==k)) <= 200){
      sub_labels[which(labels==k)] <- paste0(sub_labels[which(labels==k)], "_COHORT")
    } else {
      cohort_subset_indexes <- base::sample(which(labels==k), 200, replace = FALSE)
      case_subset_indexes <- base::setdiff(which(labels==k), cohort_subset_indexes)
      
      sub_labels[cohort_subset_indexes] <- paste0(sub_labels[cohort_subset_indexes], "_COHORT")
      sub_labels[case_subset_indexes] <- paste0(sub_labels[case_subset_indexes], "_CASE")
      
    }
  }
  merged_counts$sex_cluster_sub[merged_counts$super_cluster == i] <- paste0(i, "_", sub_labels) 
}
```


plot all sex clusters and write membership to file. 
```{r}
for(i in 1:n_super_clusters){
  current_super_cluster <- merged_counts[merged_counts$super_cluster == super_clusters[i],]
  print(ggplot(current_super_cluster, aes(x=x, y=y, color=sex_cluster)) + 
  geom_point() + 
  xlab("normalized chrX counts") + 
  ylab("normalized chrY counts") + 
  ggtitle(super_clusters[i]) )
}
memb <- data.frame(membership.sample_set_id=merged_counts$sex_cluster_sub, sample=merged_counts$sample)
table(memb$membership.sample_set_id)
write.table(memb, paste0(wd, "membership_plus_sex_cluster.tsv"), sep="\t", col.names = TRUE, row.names = FALSE, quote=FALSE)
```



update sample file and upload
```{r}



```





#Step 11
After running cohort mode, you will need to upload certain reference files so that case mode can use the model built by cohort mode
```{r}
wd <- "/path/to/folder/"
meta <- read.table(paste0(wd, "sample_set_entity.tsv"), sep="\t", header=TRUE, stringsAsFactors = FALSE)
cohorts <- meta[,1][grepl("COHORT", meta[,1])]  # change to your cohort
pse_case <- NULL
gbucket = 'gs:///000_aux/' # change to your gbucket
for(i in 1:length(cohorts)){
    cohort <- cohorts[i]
    ind <- which(meta[,1] == cohort)
    case <- str_replace(cohort, "COHORT", "CASE")
    if(case %in% meta[,1]){
        message(case)
        makeFiles("calling_configs", case, meta[ind,], gbucket)
        makeFiles("denoising_configs", case, meta[ind,], gbucket)
        makeFiles("gcnvkernel_version", case, meta[ind,], gbucket)
        makeFiles("gcnv_model_tars", case, meta[ind,], gbucket)
        makeFiles("sharded_interval_lists", case, meta[ind,], gbucket)    
        pse_case <- rbind(pse_case, meta[ind,])
    }
}
## Update the PARTICIPANT SET file
pse_case <- pse_case[, which(colnames(pse_case) %in% c("entity.sample_set_id", "intervals", "filtered_intervals", "contig_ploidy_model_tar"))]
    pse_case[,1] <- str_replace(pse_case[,1], "COHORT", "CASE")    
    pse_case$file_gcnv_model_tars <- paste0(gbucket, pse_case[,1], "-gcnv_model_tars.txt")
    pse_case$file_calling_configs <- paste0(gbucket, pse_case[,1], "-calling_configs.txt")
    pse_case$file_denoising_configs <- paste0(gbucket, pse_case[,1], "-denoising_configs.txt")
    pse_case$file_gcnvkernel_version <- paste0(gbucket, pse_case[,1], "-gcnvkernel_version.txt")
    pse_case$file_sharded_interval_lists <- paste0(gbucket, pse_case[,1], "-sharded_interval_lists.txt")        
        pse_case <- rbind(as.character(colnames(pse_case)), apply(pse_case, 2, as.character))
        pse_case[1,1] <- "entity:sample_set_id"
        write.table(pse_case, sep="\t", row.names=F, col.names=F, file=paste0(wd, "/c_pse.txt"), quote=F)
```


#Step 12
on Terra/Fireloud, run gCNV. 

#Step 13
Download the segment VCFs and unzip them. 
```{r}
wd <- "/path/to/folder/"
system2("java", sprintf("-Xmx16G -jar %s downloadSegmentsVCFs %ssample_set_entity.tsv %s genotyped_segments_vcf", jar_path, wd, wd ))
```

#Step 14
Convert VCFs to BED format
```{r}
system2("java", sprintf("-Xmx16G -jar %s convertVCFsToBEDFormat %s svtk_input.bed genotyped-segments- .vcf", jar_path, wd, wd ))
```


#Step 15
Cluster calls together. You will need to install svtk: https://github.com/talkowski-lab/svtk
```{r}
system2("svtk", sprintf("bedcluster %ssvtk_input.bed %ssvtk_output.bed", wd, wd))
```

#Step 16
Merge the input and output of svtk, as svtk will delete exra metadata fields
```{r}
system2("java", sprintf("-Xmx16G -jar %s svtkMatch %ssvtk_input.bed %ssvtk_output.bed %sclustered.bed", jar_path, wd, wd, wd))
```


#Step 17
defragment calls
```{r}
system2("java", sprintf("-Xmx16G -jar %s defragment %sclustered.bed %sdefragmented.bed", jar_path, wd, wd))
```

#Step 18
add QC labels and filters
```{r}
system2("java", sprintf("-Xmx16G -jar %s filter %sdefragmented.bed %sunannotated.bed", jar_path, wd, wd))
```

# annotate with genes

# count exons

# add gene list labels

# generate metadata

look at aneuploidies. First need to download the ploidy information
input is manifest file
#!/bin/bash

wcl=$(wc -l ${1} | cut -d " " -f1)
echo $wcl


for i in $(seq 1 ${wcl})
do
        file=$(sed ''${i}'!d' ${1})
        echo ${file}
        mkdir "ploidy_file_${i}"
        gsutil cp ${file} "./ploidy_file_${i}/ploidy_file_${i}.tar.gz"
done


#!/bin/bash
for d in $(ls -d ploidy*/)
do
echo $d
cd $d
tar -xf *tar.gz
cd ..
done

Get ploidy conunts
```{r}
library(stringr)
wd <- ""
ploidy_files <- list.files(paste0(wd ,"ploidy/"), pattern="contig_ploidy.tsv", recursive = TRUE, full.names = TRUE)

ploidy_dfs <- lapply(ploidy_files, function(x){
  df <- read.table(x, comment.char = "@", header=TRUE, stringsAsFactors=FALSE, quote="", fill=TRUE)
  df$sample <- str_extract_all(scan(x, what="character", sep="\n", nlines=1, quiet=TRUE)[[1]], "(?<=SM:).*")[[1]]
  return(df)
})

ploidy_df <- do.call(rbind, ploidy_dfs)
sex_ploidy_sums <- do.call(c, lapply(ploidy_dfs, function(df) sum(df$PLOIDY[str_detect(df$CONTIG, "X|Y")])))
sex_ploidy_samples <- unique(do.call(rbind, ploidy_dfs[which(sex_ploidy_sums!=2)])$sample)
autosome_ploidy_samples <- ploidy_df[ploidy_df$PLOIDY!=2 & !str_detect(ploidy_df$CONTIG, "X|Y"),]$sample
ploidy_samples <- c(sex_ploidy_samples, autosome_ploidy_samples)
```

Download DCR files
```{r}
# This will take a while, you might want to subset to files with aneuploidies
system2("java", sprintf("-Xmx16G -jar %s getBarcodeCounts %ssample_set_entity.tsv %s/ploidy/ denoised_copy_ratios", jar_path, wd, wd ))
```


Plot ploidy counts,
#TODO : rewrite temp1 to use lapply for with ploidy samples as input
```{r}
wd <- ""
dcr_files <- list.files(wd, pattern="denoised_copy_ratios-", recursive = TRUE, full.names = TRUE)
temp1 <- str_detect(dcr_files, paste0("(",paste(ploidy_samples, collapse=")|(") , ")")) 
temp2 <- dcr_files[temp1]

cluster <- makeCluster(detectCores()-1)
clusterExport(cl=cluster, varlist=c("temp2", "str_detect", "str_extract"))
dcr_dfs <- parLapply(cluster, temp2, function(x){
  lines <- scan(x, what="character", sep="\n", quiet=TRUE)
  df <- read.table(textConnection(lines), comment.char = "@", header=TRUE, stringsAsFactors=FALSE, quote="", fill=TRUE)
  df$sample <- str_extract(lines[str_detect(lines, "SM:")], "(?<=SM:).*")[[1]]
  return(df)
})
stopCluster(cluster)
dcr_df <- do.call(rbind, dcr_dfs)

#plot linear copy ratio over chromosome for autosomes
autosome_ploidy_dfs <- ploidy_df[ploidy_df$PLOIDY!=2 & (ploidy_df$sample %in% autosome_ploidy_samples) & !str_detect(ploidy_df$CONTIG, "X|Y"), ]
for(i in 1:nrow(autosome_ploidy_dfs)){
  df <- dcr_df[dcr_df$sample==autosome_ploidy_dfs$sample[i] & dcr_df$CONTIG==autosome_ploidy_dfs$CONTIG[i], ]
  print(plot_ploidy(df))
  pdf(sprintf("%sploidy_sample_%s_%s.pdf", wd, df$sample[1], df$CONTIG[1]))
  my_plot <- plot_ploidy(df)
  print(my_plot)
  dev.off()
}

sex_ploidy_dfs <- ploidy_df[(ploidy_df$sample %in% sex_ploidy_samples) & str_detect(ploidy_df$CONTIG, "X|Y"), ]
df_instances <- rep("-1", unique(sex_ploidy_dfs$sample) %>% length)
for(i in seq_along(unique(sex_ploidy_dfs$sample))){
  par(mfrow=c(1,2))
  df <- dcr_df[dcr_df$sample==unique(sex_ploidy_dfs$sample)[i], ]
  # plot_x <- plot_ploidy(df[str_detect(df$CONTIG, "X"),])
  # plot_y <- plot_ploidy(df[str_detect(df$CONTIG, "Y"),])
  # grid.arrange(plot_x, plot_y, ncol=2)
  
  temp_df_x <- df[str_detect(df$CONTIG, "X"),]
  temp_df_y <- df[str_detect(df$CONTIG, "Y"),]
  df_instances[i] <- c(
                          rep(temp_df_x$CONTIG[1], ploidy_df[ploidy_df$CONTIG==temp_df_x$CONTIG[1] & ploidy_df$sample==temp_df_x$sample[1],]$PLOIDY[1]),
                          rep(temp_df_y$CONTIG[1], ploidy_df[ploidy_df$CONTIG==temp_df_y$CONTIG[1] & ploidy_df$sample==temp_df_y$sample[1],]$PLOIDY[1])
                          ) %>% paste0(collapse = "")
# 
#   my_plot <- arrangeGrob(plot_x, plot_y, ncol=2)
#   ggsave(sprintf("%s/dcr_plots/ploidy_sample_%s_sex_chroms.pdf", wd, df$sample[1]), my_plot)
}
```


```{r}
case_1 <- df_d$sample[df_d$batch=="cluster_1_CASE"] %>% unique
cohort_1 <- df_d$sample[df_d$batch=="cluster_1_COHORT"] %>% unique

case_instances <- rep("-1", length(case_1))
cohort_instances <- rep("-1", length(cohort_1))

for(i in seq_along(case_1)){
  case_instances[i] <- c(rep(ploidy_df$CONTIG[ploidy_df$sample==case_1[i]][23], ploidy_df$PLOIDY[ploidy_df$sample==case_1[i]][23]), 
                         rep(ploidy_df$CONTIG[ploidy_df$sample==case_1[i]][24], ploidy_df$PLOIDY[ploidy_df$sample==case_1[i]][24])
                         ) %>% paste0(collapse = "")
  
}

for(i in seq_along(cohort_1)){
  cohort_instances[i] <- c(rep(ploidy_df$CONTIG[ploidy_df$sample==cohort_1[i]][23], ploidy_df$PLOIDY[ploidy_df$sample==cohort_1[i]][23]), 
                         rep(ploidy_df$CONTIG[ploidy_df$sample==cohort_1[i]][24], ploidy_df$PLOIDY[ploidy_df$sample==cohort_1[i]][24])
                         ) %>% paste0(collapse = "")
  
}

all_instances <- rep("-1", length(ploidy_df$sample %>% unique))
all_samples <- ploidy_df$sample %>% unique
for(i in seq_along(all_samples)){
  all_instances[i] <- c(rep(ploidy_df$CONTIG[ploidy_df$sample==all_samples[i]][23], ploidy_df$PLOIDY[ploidy_df$sample==all_samples[i]][23]), 
                         rep(ploidy_df$CONTIG[ploidy_df$sample==all_samples[i]][24], ploidy_df$PLOIDY[ploidy_df$sample==all_samples[i]][24])
                         ) %>% paste0(collapse = "")
  
}
```



Main Figures
variant count boxplots
```{r}
wd <- ""
gcnv <- read(wd, "final_callset_all_strictCountedExons.tsv")

qs_window <- seq(0, 100, 10)
total_variants <- rep(0, length(qs_window))
variant_counts <- vector("list", length(qs_window))

for(i in seq_along(qs_window)){
  filtered_qs <- gcnv$QS >= qs_window[i]
  filtered_vafs <- gcnv$site_frequency <= 0.01
  filtered <- gcnv[filtered_qs & filtered_vafs ,]
  total_variants[i] <- length(unique(filtered$name))
  variant_counts[[i]] <- as.numeric(table(filtered$sample))
}

n_outliers <- lapply(variant_counts, function(x) length(boxplot(x, plot=FALSE)$out) )

lengths <- sapply(variant_counts, length)
labels <- lapply(seq_along(qs_window), function(x){rep(qs_window[x], lengths[x])}) %>% unlist %>% as.character()

temp1 <- letters[seq_along(qs_window)]
labels2 <- lapply(seq_along(temp1), function(x){rep(temp1[x], lengths[x])}) %>% unlist
df <- data.frame(qs_threshold=labels, n_variants=unlist(variant_counts), class=labels2)
df$qs_threshold <- factor(df$qs_threshold, levels=qs_window)

df$n_variants_log2 <- (df$n_variants) %>% log2
df$n_variants_log10 <- df$n_variants %>% log10

gcnvhq <- gcnv[gcnv$PASS_freq & gcnv$PASS_QS & gcnv$PASS_sample, ]
#write.table(gcnvhq, paste0(wd, "gcnv_HQ.tsv"), sep="\t", row.names = FALSE, col.names = TRUE, quote=FALSE)
```


```{r}
g1 <- ggplot(df, aes(x=qs_threshold, y=n_variants)) +
  geom_boxplot(outlier.size = NA, outlier.shape = NA, color=wes_palette("Zissou1")[1], fill=wes_palette("Zissou1")[2], alpha=0.6) + 
  coord_cartesian(ylim = c(0, 40)) +
  xlab("QS threshold") + 
  ylab("# of Variants per sample") +
  ggtitle("Variants per sample, outliers not shown") +
  theme(plot.title = element_text(hjust = 0.5))
g1
```


```{r}
g2 <- ggplot(df[df$qs_threshold!=0 & df$qs_threshold!=10,], aes(x=qs_threshold, y=n_variants)) +
  geom_boxplot(outlier.size = NA, outlier.shape = NA, color=wes_palette("Zissou1")[1], fill=wes_palette("Zissou1")[2], alpha=0.6) + 
  coord_cartesian(ylim = c(0, 16)) +
  xlab("QS threshold") + 
  ylab("# of Variants per sample") +
  ggtitle("Variants per sample, outliers not shown") +
  theme(plot.title = element_text(hjust = 0.5))
g2
```

variant count histograms
```{r}
g3 <- ggplot(df[df$qs_threshold==0,] , aes(x=n_variants_log2)) +
  geom_histogram(aes(y=..density..), colour="BLACK", fill=wes_palette("FantasticFox1")[1]) + 
  geom_density(alpha=.2, bw=0.3, size=1, color=wes_palette("FantasticFox1")[5]) +
  ylab("frequency") + 
  xlab("number of variants") +
  ggtitle(bquote(~log[2]~"transformed number of variants; QS threshold=0 ")) +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_x_continuous(breaks=0:10, labels=2^(0:10))
g3
```

```{r}
bks <- seq(0, 10, 0.6)
g4 <- ggplot(df[df$qs_threshold==50,] , aes(x=n_variants_log2)) +
  geom_histogram(aes(y=..density..), colour="BLACK", fill=wes_palette("FantasticFox1")[1], breaks=bks) + 
  geom_density(alpha=.2, bw=0.5, size=1, color=wes_palette("FantasticFox1")[5]) +
  ylab("frequency") + 
  xlab("number of variants") +
  ggtitle(bquote(~log[2]~"transformed number of variants; QS threshold=50 ")) +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_x_continuous(breaks=0:10, labels=2^(0:10))
g4
```


```{r}
bks <- seq(0, 10, 0.6)
g5 <- ggplot(df[df$qs_threshold==100,] , aes(x=n_variants_log2)) +
  geom_histogram(aes(y=..density..), colour="BLACK", fill=wes_palette("FantasticFox1")[1], breaks=bks) + 
  geom_density(alpha=.2, bw=0.9, size=1, color=wes_palette("FantasticFox1")[5]) +
  ylab("frequency") + 
  xlab("number of variants") +
  ggtitle(bquote(~log[2]~"transformed number of variants; QS threshold=100 ")) +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_x_continuous(breaks=0:10, labels=2^(0:10))
g5
```


get counts of CNVS per bin
bedtools intersect -wa -a gencode_v33_split.bed -b gcnv_final.tsv |  awk '{print $1 "-" $2}' > intersections.bed
```{r}
if(FALSE){
  del_intersections <- read.table(paste0(wd, "intersections_all_DEL.tsv"), sep="\t", header=FALSE, stringsAsFactors = FALSE)
  dup_intersections <- read.table(paste0(wd, "intersections_all_DUP.tsv"), sep="\t", header=FALSE, stringsAsFactors = FALSE)
  var_type <- "raw"
  
  count_samples_per_cnv <- table(gcnv$name)
  df_count_samples_per_cnv <- data.frame(name=names(count_samples_per_cnv), sample_Count= as.numeric(count_samples_per_cnv))
  variant_df <- gcnv[match(unique(gcnv$name), gcnv$name), c(1,2,4,5, 18, 19, 20)] %>% merge(df_count_samples_per_cnv)
  variant_df$bp <- variant_df$start
  variant_df$start <- NULL
  variant_df_del <- variant_df[variant_df$svtype=="DEL", ]
  variant_df_dup <- variant_df[variant_df$svtype=="DUP", ]
} else {
  del_intersections <- read.table(paste0(wd, "intersections_HQ_DEL.tsv"), sep="\t", header=FALSE, stringsAsFactors = FALSE)
  dup_intersections <- read.table(paste0(wd, "intersections_HQ_DUP.tsv"), sep="\t", header=FALSE, stringsAsFactors = FALSE)
  var_type <- "HQ"
  
  gcnvhq <- gcnv[gcnv$HIGH_QUALITY==TRUE, ]
  count_samples_per_cnv <- table(gcnvhq$name)
  df_count_samples_per_cnv <- data.frame(name=names(count_samples_per_cnv), sample_Count= as.numeric(count_samples_per_cnv))
  variant_df <- gcnvhq[match(unique(gcnvhq$name), gcnvhq$name), c(1,2,4,5, 18, 19, 20)] %>% merge(df_count_samples_per_cnv)
  variant_df$bp <- variant_df$start
  variant_df$start <- NULL
  variant_df_del <- variant_df[variant_df$svtype=="DEL", ]
  variant_df_dup <- variant_df[variant_df$svtype=="DUP", ]
}

del_freq <- table(del_intersections$V1)
del_df <- data.frame(name=names(del_freq), count=as.numeric(del_freq))
del_df$chr <- del_df$name %>% str_extract(".*(?=-)") 
del_df$bp <- del_df$name %>% str_extract_all("(?<=-).*") %>% as.numeric()

dup_freq <- table(dup_intersections$V1)
dup_df <- data.frame(name=names(dup_freq), count=as.numeric(dup_freq))
dup_df$chr <- dup_df$name %>% str_extract(".*(?=-)")
dup_df$bp <- dup_df$name %>% str_extract_all("(?<=-).*") %>% as.numeric()

```

#percent of cnvs per bin, for each bin, what percentage of cnv calls overlap it?
```{r}
out_dir <- paste0("CNV_per_bin_percent_local-scale_", var_type, "/")
dir.create(paste0(wd, out_dir))
lapply(unique(del_df$chr), function(x){
  temp_df_del <- del_df[del_df$chr==x,]
  temp_df_dup <- dup_df[dup_df$chr==x,]
  sum_del <- sum(temp_df_del$count)
  sum_dup <- sum(temp_df_dup$count)
  sum_all <- sum_del + sum_dup
  temp_df_del$percent_count <- temp_df_del$count / sum_all * 100
  temp_df_dup$percent_count <- temp_df_dup$count / sum_all * 100
  ymax=max(c(max(temp_df_del$percent_count, max(temp_df_dup$percent_count)))) * 1.05
  
  g_del <- ggplot(temp_df_del, aes(x=bp, y=percent_count)) +
    geom_point(colour=wes_palette("FantasticFox1")[5]) +
    theme(legend.position="none", plot.title = element_text(hjust = 0.5)) +
    xlab("coordinates") +
    ggtitle(paste0(x, " DEL")) + 
    ylim(0, ymax) + 
    ylab(paste0("percent of ", var_type," CNVs per bin, ", x))
  
  g_dup <- ggplot(temp_df_dup, aes(x=bp, y=percent_count)) +
    geom_point(colour=wes_palette("FantasticFox1")[3]) +
    theme(legend.position="none", plot.title = element_text(hjust = 0.5)
          , axis.title.y=element_blank(),) +
    xlab("coordinates") +
    ylim(0, ymax) + 
    ggtitle(paste0(x, " DUP"))
  
  png(paste0(wd, out_dir, "CNV_per_bin_percent_local-scale_", x, "_", var_type,".png"), width=14, height=7, units="in", res=300)
  grid.arrange(g_del, g_dup, ncol=2)
  dev.off()
})
```

#GLOBAL
```{r}
out_dir <- paste0("CNV_per_bin_count_global-scale_", var_type,"/")
dir.create(paste0(wd, out_dir))
ymax=max(c(max(del_df$count, max(del_df$count)))) * 1.05
lapply(unique(del_df$chr), function(x){
  temp_df_del <- del_df[del_df$chr==x,]
  temp_df_dup <- dup_df[dup_df$chr==x,]
  
  g_del <- ggplot(temp_df_del, aes(x=bp, y=count)) +
    geom_point(colour=wes_palette("FantasticFox1")[5]) +
    theme(legend.position="none", plot.title = element_text(hjust = 0.5)) +
    xlab("coordinates") +
    ylim(0, ymax) + 
    ggtitle(paste0(x, " DEL")) + 
    ylab(paste0("count of ", var_type," CNVs per bin, ", x))
  
  g_dup <- ggplot(temp_df_dup, aes(x=bp, y=count)) +
    geom_point(colour=wes_palette("FantasticFox1")[3]) +
    theme(legend.position="none", plot.title = element_text(hjust = 0.5)
          , axis.title.y=element_blank(),) +
    xlab("coordinates") +
    ylim(0, ymax) + 
    ggtitle(paste0(x, " DUP"))
  
  png(paste0(wd, out_dir, "CNV_per_bin_count_global-scale_", x, "_", var_type,".png"), width=14, height=7, units="in", res=300)
  grid.arrange(g_del, g_dup, ncol=2)
  dev.off()
})
```

#LOCAL
```{r}
out_dir <- paste0("CNV_per_bin_count_local-scale_", var_type, "/")
dir.create(paste0(wd, out_dir))
lapply(unique(del_df$chr), function(x){
  temp_df_del <- del_df[del_df$chr==x,]
  temp_df_dup <- dup_df[dup_df$chr==x,]
  ymax=max(c(max(temp_df_del$count, max(temp_df_dup$count)))) * 1.05
  
  g_del <- ggplot(temp_df_del, aes(x=bp, y=count)) +
    geom_point(colour=wes_palette("FantasticFox1")[5]) +
    theme(legend.position="none", plot.title = element_text(hjust = 0.5)) +
    xlab("coordinates") +
    ylim(0, ymax) + 
    ggtitle(paste0(x, " DEL")) + 
    ylab(paste0("count of ", var_type," CNVs per bin, ", x))
  
  g_dup <- ggplot(temp_df_dup, aes(x=bp, y=count)) +
    geom_point(colour=wes_palette("FantasticFox1")[3]) +
    theme(legend.position="none", plot.title = element_text(hjust = 0.5)
          , axis.title.y=element_blank(),) +
    xlab("coordinates") +
    ylim(0, ymax) + 
    ggtitle(paste0(x, " DUP"))
  
  png(paste0(wd, out_dir, "CNV_per_bin_count_local-scale_", x, "_", var_type,".png"), width=14, height=7, units="in", res=300)
  grid.arrange(g_del, g_dup, ncol=2)
  dev.off()
})
```

#percent of samples per bin, for each bin, what percentage of samples have a CNV call overlapping it?
```{r}
out_dir <- paste0("sample_per_bin_percent_local-scale_", var_type,"/")
dir.create(paste0(wd, out_dir))
lapply(unique(del_df$chr), function(x){
  temp_df_del <- variant_df_del[variant_df_del$chr==x,]
  temp_df_dup <- variant_df_dup[variant_df_dup$chr==x,]
  sum_del <- sum(temp_df_del$sample_Count)
  sum_dup <- sum(temp_df_dup$sample_Count)
  sum_all <- sum_del + sum_dup
  temp_df_del$percent_count <- temp_df_del$sample_Count / sum_all * 100
  temp_df_dup$percent_count <- temp_df_dup$sample_Count / sum_all * 100
  ymax=max(c(max(temp_df_del$percent_count, max(temp_df_dup$percent_count)))) * 1.05
  
  #print(paste(x, dim(temp_df_del), dim(temp_df_dup)))
  
  g_del <- ggplot(temp_df_del, aes(x=bp, y=percent_count)) +
    geom_point(colour=wes_palette("FantasticFox1")[5]) +
    theme(legend.position="none", plot.title = element_text(hjust = 0.5)) +
    xlab("coordinates") +
    ggtitle(paste0(x, " DEL")) + 
    ylim(0, ymax) + 
    ylab(paste0("percent of samples with ", var_type," CNVs per bin, ", x))
  
  g_dup <- ggplot(temp_df_dup, aes(x=bp, y=percent_count)) +
    geom_point(colour=wes_palette("FantasticFox1")[3]) +
    theme(legend.position="none", plot.title = element_text(hjust = 0.5)
          , axis.title.y=element_blank(),) +
    xlab("coordinates") +
    ylim(0, ymax) + 
    ggtitle(paste0(x, " DUP"))
  
  png(paste0(wd, out_dir, "percent_samples_per_bin_percent_local-scale_", x, "_", var_type,".png"), width=14, height=7, units="in", res=300)
  grid.arrange(g_del, g_dup, ncol=2)
  dev.off()
})
```




```{r}
jar_path <- "C:/Users/iwong/Documents/MGH/scripts/gCNV_helper.jar"
system2("java", sprintf("-Xmx16G -jar %s --help", jar_path))
system2("java", sprintf("-Xmx16G -jar %s getPerSampleMetrics  %sfinal_callset_all_strictCountedExons.tsv name %sby_variant.tsv", jar_path, wd, wd ))
system2("java", sprintf("-Xmx16G -jar %s getPerSampleMetrics  %sfinal_callset_HQ_strictCountedExons.tsv name %sby_HQ_variant.tsv", jar_path, wd, wd ))
system2("java", sprintf("-Xmx16G -jar %s getPerSampleMetrics  %sfinal_callset_HQ_strictCountedExons.tsv sample %sby_HQ_sample.tsv", jar_path, wd, wd ))
```
```{r}
variant_df <- read.table(paste0(wd, "by_HQ_sample.tsv"), sep="\t", header=TRUE, stringsAsFactors = FALSE, fill=TRUE)
variant_df$X <- NULL

#df1 <- gcnv[match(unique(gcnv$name), gcnv$name),c(1,2,4,5,6)] %>% merge(variant_df)
df1 <- gcnv[match(unique(gcnv$sample), gcnv$sample),c(1,2,4,5,6)] %>% merge(variant_df)

df1$bp <- df1$start
dup_df <- df1[df1$svtype=="DUP", ]
del_df <- df1[df1$svtype=="DEL", ]
```


```{r}
out_dir <- paste0("QS_MEDIAN_HQ_sample/")
dir.create(paste0(wd, out_dir))
column_to_plot <- "QS_MEDIAN"
lapply(unique(del_df$chr), function(x){
  temp_df_del <- del_df[del_df$chr==x,]
  temp_df_dup <- dup_df[dup_df$chr==x,]
  ymax=max(c(max(temp_df_del[column_to_plot], max(temp_df_dup[column_to_plot])))) * 1.05
  
  g_del <- ggplot(temp_df_del, aes(x=bp, y=QS_MEDIAN)) +
    geom_point(colour=wes_palette("FantasticFox1")[5]) +
    theme(legend.position="none", plot.title = element_text(hjust = 0.5)) +
    xlab("coordinates") +
    ylim(0, ymax) + 
    ggtitle(paste0(x, " DEL")) + 
    scale_y_continuous(trans = 'log2')    
  
  g_dup <- ggplot(temp_df_del, aes(x=bp, y=QS_MEDIAN)) +
    geom_point(colour=wes_palette("FantasticFox1")[3]) +
    theme(legend.position="none", plot.title = element_text(hjust = 0.5)
          , axis.title.y=element_blank(),) +
    xlab("coordinates") +
    ylim(0, ymax) + 
    ggtitle(paste0(x, " DUP")) +
    scale_y_continuous(trans = 'log2')  
  
  png(paste0(wd, out_dir, column_to_plot, "_local-scale_", x, "_HQ_sample.png"), width=14, height=7, units="in", res=300)
  grid.arrange(g_del, g_dup, ncol=2)
  dev.off()
})
```

```{r}
out_dir <- paste0("QS_MEAN_HQ_sample/")
dir.create(paste0(wd, out_dir))
column_to_plot <- "QS_MEAN"
lapply(unique(del_df$chr), function(x){
  temp_df_del <- del_df[del_df$chr==x,]
  temp_df_dup <- dup_df[dup_df$chr==x,]
  ymax=max(c(max(temp_df_del[column_to_plot], max(temp_df_dup[column_to_plot])))) * 1.05
  
  g_del <- ggplot(temp_df_del, aes(x=bp, y=QS_MEAN)) +
    geom_point(colour=wes_palette("FantasticFox1")[5]) +
    theme(legend.position="none", plot.title = element_text(hjust = 0.5)) +
    xlab("coordinates") +
    ylim(0, ymax) + 
    ggtitle(paste0(x, " DEL")) + 
    scale_y_continuous(trans = 'log2')    
  
  g_dup <- ggplot(temp_df_del, aes(x=bp, y=QS_MEAN)) +
    geom_point(colour=wes_palette("FantasticFox1")[3]) +
    theme(legend.position="none", plot.title = element_text(hjust = 0.5)
          , axis.title.y=element_blank(),) +
    xlab("coordinates") +
    ylim(0, ymax) + 
    ggtitle(paste0(x, " DUP")) +
    scale_y_continuous(trans = 'log2')  
  
  png(paste0(wd, out_dir,column_to_plot, "_local-scale_", x, "_HQ_sample.png"), width=14, height=7, units="in", res=300)
  grid.arrange(g_del, g_dup, ncol=2)
  dev.off()
})
```



number of variants vs number of exons
bedtools intersect -c -a gcnv.tsv -b aux_gencode_v33_split_800.bed > gcnv_with_count.tsv
```{r}
dup_count <- read.table(paste0(wd, "gcnv_with_count_dup.tsv"), sep="\t", header=FALSE, stringsAsFactors = FALSE)
del_count <- read.table(paste0(wd, "gcnv_with_count_del.tsv"), sep="\t", header=FALSE, stringsAsFactors = FALSE)

# dup_count <- read.table(paste0(wd, "gcnv_count_HQ_dup.tsv"), sep="\t", header=FALSE, stringsAsFactors = FALSE)
# del_count <- read.table(paste0(wd, "gcnv_count_HQ_del.tsv"), sep="\t", header=FALSE, stringsAsFactors = FALSE)

colnames(dup_count) <- c(colnames(gcnv), "n_exons")
colnames(del_count) <- c(colnames(gcnv), "n_exons")

unique_dup <- dup_count[match(unique(dup_count$name), dup_count$name), ]
unique_del <- del_count[match(unique(del_count$name), del_count$name), ]

exon_table_dup <- table(unique_dup$n_exons)
exon_table_del <- table(unique_del$n_exons)

df_dup <- data.frame(nexons=names(exon_table_dup) %>% as.numeric(), nvariants=as.numeric(exon_table_dup))
df_dup <- df_dup[order(df_dup$nexons, decreasing = FALSE),]
for(i in seq(nrow(df_dup)-1, 1, -1)){
  df_dup$nvariants[i] = df_dup$nvariants[i] + df_dup$nvariants[i+1]
}

df_del <- data.frame(nexons=names(exon_table_del) %>% as.numeric(), nvariants=as.numeric(exon_table_del))
df_del <- df_del[order(df_del$nexons, decreasing = FALSE),]
for(i in seq(nrow(df_del)-1, 1, -1)){
  df_del$nvariants[i] = df_del$nvariants[i] + df_del$nvariants[i+1]
}

df_del$type <- "DEL"
df_dup$type <- "DUP"
df <- rbind(df_del, df_dup)
df$log2_nvars <- log2(df$nvariants)
df$log2_nexons <- log2(df$nexons)
```


```{r}
g6 <- ggplot(data=df[df$nexons<5000, ], aes(x=log2_nexons, y=nvariants, group=type)) +
  geom_line(aes(colour=type), size=1.5) +
  scale_color_manual(values=c(wes_palette("FantasticFox1")[5], wes_palette("FantasticFox1")[3])) +
  theme(legend.position="bottom") + 
  scale_x_continuous(breaks=0:12, labels=2^(0:12)) +
  xlab("number of exons") + 
  ylab("number of variants") + 
  ggtitle("Distribution of HQ CNVs")
g6
```

proprtion of singletons
```{r}
singletons_del <- which(table(del_count$name[!del_count$sample %in% ploidy_samples])==1) %>% names
df_single_del <- del_count[del_count$name %in% singletons_del, ]
exon_table_single_del <- table(df_single_del$n_exons)
exon_table_del <- table(del_count$n_exons)
df_del <- data.frame(nexons=names(exon_table_del) %>% as.numeric(), nvariants=as.numeric(exon_table_del))
df_del <- df_del[order(df_del$nexons, decreasing = FALSE),]
df1_del <- data.frame(nexons=names(exon_table_single_del) %>% as.numeric(), nvariants_single=as.numeric(exon_table_single_del))
df1_del <- df1_del[order(df1_del$nexons, decreasing = FALSE),]
df2_del <- merge(df_del, df1_del)
for(i in seq(nrow(df2_del)-1, 1, -1)){
  df2_del$nvariants[i] = df2_del$nvariants[i] + df2_del$nvariants[i+1]
  df2_del$nvariants_single[i] = df2_del$nvariants_single[i] + df2_del$nvariants_single[i+1]
}
df2_del$proportion <- df2_del$nvariants_single/df2_del$nvariants*100
df2_del$type <- "DEL"

singletons_dup <- which(table(dup_count$name[!dup_count$sample %in% ploidy_samples])==1) %>% names
df_single_dup <- dup_count[dup_count$name %in% singletons_dup, ]
exon_table_single_dup <- table(df_single_dup$n_exons)
exon_table_dup <- table(dup_count$n_exons)
df_dup <- data.frame(nexons=names(exon_table_dup) %>% as.numeric(), nvariants=as.numeric(exon_table_dup))
df_dup <- df_dup[order(df_dup$nexons, decreasing = FALSE),]
df1_dup <- data.frame(nexons=names(exon_table_single_dup) %>% as.numeric(), nvariants_single=as.numeric(exon_table_single_dup))
df1_dup <- df1_dup[order(df1_dup$nexons, decreasing = FALSE),]
df2_dup <- merge(df_dup, df1_dup)
for(i in seq(nrow(df2_dup)-1, 1, -1)){
  df2_dup$nvariants[i] = df2_dup$nvariants[i] + df2_dup$nvariants[i+1]
  df2_dup$nvariants_single[i] = df2_dup$nvariants_single[i] + df2_dup$nvariants_single[i+1]
}
df2_dup$proportion <- df2_dup$nvariants_single/df2_dup$nvariants*100
df2_dup$type <- "DUP"

df3 <- rbind(df2_del, df2_dup)
df3$log2_nexons <- df3$nexons %>% log2
```


```{r}
g7 <- ggplot(data=df3[df3$nexons<5000  , ], aes(x=log2_nexons, y=proportion, group=type)) +
  geom_line(aes(colour=type), size=1.5) +
  scale_color_manual(values=c(wes_palette("FantasticFox1")[5], wes_palette("FantasticFox1")[3])) +
  theme(legend.position="bottom") + 
  scale_x_continuous(breaks=0:12, labels=2^(0:12)) +
  xlab("number of exons") + 
  ylab("percent of singletons") + 
  ggtitle("Distribution of raw CNVs") + 
  ylim(0, 100) 
g7
```




overlapping histogram of size
```{r}
df <- gcnv[, c("start", "end", "svtype", "PASS_FREQ", "PASS_sample", "PASS_QS")]
df$size <- (df$end - df$start)/1000
df$size_log10 <- df$size %>% log10
df$size_log2 <- df$size %>% log2
df$HQ <- df$PASS_FREQ & df$PASS_QS & df$PASS_sample
```


```{r}
g8 <- ggplot(data=df[df$size>0.125 & df$size<1500, ], aes(x=size_log2, fill=svtype)) +
  geom_histogram(alpha=0.7, position = "identity", aes(y = ..density..)) +
  scale_fill_manual(values=c(wes_palette("FantasticFox1")[5], wes_palette("FantasticFox1")[3])) +
  scale_x_continuous(breaks=-4:10, labels = 2^(-4:10)) + 
  xlab("CNV size (KB)") + 
  ggtitle("Raw CNV size distribution") 
g8
```

```{r}
g9 <- ggplot(data=df[df$size>0.125 & df$size<1500 & df$HQ, ], aes(x=size_log2, fill=svtype)) +
  geom_histogram(alpha=0.7, position = "identity", aes(y = ..density..)) +
  scale_fill_manual(values=c(wes_palette("FantasticFox1")[5], wes_palette("FantasticFox1")[3])) +
  scale_x_continuous(breaks=-4:10, labels = 2^(-4:10)) + 
  xlab("CNV size (KB)") + 
  ggtitle("HQ CNV size distribution")
g9
```

venn diagram

```{r}
# wd <- ""
# gcnv <- read(wd, "final_callset.tsv")

samp <- c(TRUE, FALSE)
freq <- c(TRUE, FALSE)
qs <- c(TRUE, FALSE)
temp_df <- expand.grid(samp, freq, qs)
colnames(temp_df) <- c("samp", "freq","qs")
temp_df$count <- -1
temp_df$percent_of_all_calls <- -1
temp_df$per_sample <- -1
for(i in 1:nrow(temp_df)){
  temp_df$count[i] <- nrow(gcnv[gcnv$PASS_sample==temp_df$samp[i] & gcnv$PASS_FREQ==temp_df$freq[i] & gcnv$PASS_QS==temp_df$qs[i],])
  temp_df$percent_of_all_calls[i] <- temp_df$count[i] / nrow(gcnv) * 100
  temp_df$per_sample[i] <- temp_df$count[i] / (gcnv$sample %>% unique %>% length)
}


length(unique(gcnv[gcnv$PASS_sample==temp_df$samp[i] & gcnv$PASS_freq==temp_df$freq[i] & gcnv$PASS_QS==temp_df$qs[i],]$sample))
dim(gcnv[gcnv$PASS_sample==temp_df$samp[i] & gcnv$PASS_freq==temp_df$freq[i] & gcnv$PASS_QS==temp_df$qs[i],])
length(unique(gcnv[gcnv$PASS_sample==temp_df$samp[i] & gcnv$PASS_freq==temp_df$freq[i] & gcnv$PASS_QS==temp_df$qs[i],]$name))

length(unique(gcnv$sample))
dim(gcnv)
length(unique(gcnv$name))

temp_df

pass_qs <- paste0(gcnv$sample[gcnv$PASS_QS], gcnv$name[gcnv$PASS_QS])
pass_freq <- paste0(gcnv$sample[gcnv$PASS_FREQ], gcnv$name[gcnv$PASS_FREQ])
pass_samp <- paste0(gcnv$sample[gcnv$PASS_sample], gcnv$name[gcnv$PASS_sample])

library(RColorBrewer)
myCol <- brewer.pal(3, "Pastel2")
myCol <- wes_palette("Darjeeling1")[1:3]
library(VennDiagram)
venn.diagram(
  x = list(pass_qs, pass_freq, pass_samp),
  category.names = c("pass_qs (32,673)" , "pass_freq (213,474)" , "pass_sample (62,678)"),
  filename = paste0(wd, 'venn_diagramm.png'),
  output=TRUE,
  print.mode = c("raw", "percent"),
  
  # Output features
  imagetype="png" ,
  height = 900 , 
  width = 900 , 
  resolution = 300,
  compression = "lzw",
  
  # Circles
  lwd = 2,
  lty = 'blank',
  fill = myCol,
  
  # Numbers
  cex = .6,
  fontface = "bold",
  fontfamily = "sans",
  
  # Set names
  cat.cex = 0.6,
  cat.fontface = "bold",
  cat.default.pos = "outer",
  cat.pos = c(-27, 27, 135),
  cat.dist = c(0.055, 0.055, 0.085),
  cat.fontfamily = "sans",
  rotation = 1
)

```

