---
title: "gCNV_Pipeline"
output: html_document
author: Isaac Wong
version: 2.10
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

This workflow is an attempt to provide instructions on running gCNV. Please let me know of any isses you run into or any processes you want improved/automated. Please ensure you have downloaded the most recent files from my github: https://github.com/theisaacwong/talkowski/tree/master/gCNV. You should be able to run the pipeline chunk by chunk.Basic GATK is below:
```{}
        gatk --java-options "-Xmx${command_mem_mb}m" CollectReadCounts \
            -I ${cram} \
            --read-index ${crai} \
            -L ${intervals_barcode} \
            --interval-merging-rule OVERLAPPING_ONLY \
            --reference ${hg38_reference} \
            --format TSV \
            -O ${counts_barcode_filename}
```


#Step 0
Call GATK's CollectReadcounts on bam files to get interval coverage. (https://gatk.broadinstitute.org/hc/en-us/articles/360036729331-CollectReadCounts).Bsst practice is to batch samples into groups of ~400 and run the WDL on each batch separately. 

Here is a WDL to run it: https://portal.firecloud.org/?return=terra#methods/countingMethods/countingCrams/8/wdl


To begin, load the required packages and some functions
```{r}
library("stringr")
library("rgl")
library("factoextra")
library("tidyverse")
library("clusterSim")
library("cluster")
library("clValid")
library("GenomicRanges")
library("parallel")
library("gridExtra")
library("wesanderson")
library("ggplot2")


srt <- function(x){
  sort(table(x), decreasing = TRUE)
}

deFragment <- function(tab, bins, extension=0.3){
    tab$defragged <- FALSE
    tab$cprotect <- FALSE    
    gr_tab <- GRanges(paste0(tab$sample, "::::", tab$chr), IRanges(tab$start, tab$end))
    bs_tab <- toBinSpace(gr_tab, bins, pattern="::::")
    values(bs_tab) <- tab    
    wid_bs <- width(bs_tab)
    bs_tab_ext <- GRanges(seqnames(bs_tab), IRanges(start(bs_tab) - round(wid_bs*extension), end(bs_tab) + round(wid_bs*extension)))
    values(bs_tab_ext) <- values(bs_tab)    ### Find overlaps now produced by extension
    ol <- findOverlaps(bs_tab_ext, bs_tab_ext)
    ol <- ol[-which(queryHits(ol)==subjectHits(ol))]    ### Extract CN of overlaps [If there is overlap introduced by extension]
    if(length(ol)>0){
      pairing <- unique(t(apply(cbind(queryHits(ol), subjectHits(ol)), 1, sort)))
      pairing <- cbind(pairing, bs_tab_ext$CN[pairing[,1]], bs_tab_ext$CN[pairing[,2]])
      ### Subset to pairs of compatible CN when extended
      paired <- pairing[pairing[,3]==pairing[,4],, drop=FALSE]        
      if(dim(paired)[1]>0){
        ### Create merged-pair with un-extended boundaries
        bs_paired <- GRanges(seqnames(bs_tab)[as.numeric(paired[,1])], 
                             IRanges(apply(cbind(start(bs_tab[as.numeric(paired[,1])]), start(bs_tab[as.numeric(paired[,2])])), 1, min),
                                     apply(cbind(end(bs_tab[as.numeric(paired[,1])]), end(bs_tab[as.numeric(paired[,2])])), 1, max)))
        bs_paired$CN <- bs_tab_ext$CN[as.numeric(paired[,1])]            ### Check that merged-pair does not bulldoze inconsistent CN calls in the middle
        ol_bull <- findOverlaps(bs_paired, bs_tab)
        clobber <- sapply(1:length(bs_paired), function(x){
          sum(bs_tab$CN[subjectHits(ol_bull)[queryHits(ol_bull)==x]] != bs_paired$CN[x])
        })
        protect <- sapply(unique(subjectHits(ol_bull)), function(x){
          sum(bs_tab$CN[x] != bs_paired$CN[unique(queryHits(ol_bull)[subjectHits(ol_bull)==x])])
        })
        ### Annotate for when there was clobber protection
        tab$cprotect[unique(subjectHits(ol_bull))] <- protect>0            ### Reduce non-bulldozing pairs
        bs_paired_noclobber <- bs_paired[clobber==0]
        bs_paired_red <- GenomicRanges::reduce(bs_paired_noclobber)            ### Re-insert call-level information
        ol_reinsert <- GenomicRanges::findOverlaps(bs_paired_red, bs_tab)
        replacements <- do.call(rbind, sapply(1:length(bs_paired_red), function(x){
          vals <- values(bs_tab)[subjectHits(ol_reinsert)[queryHits(ol_reinsert)==x],]
          vals <- vals[order(vals$start),]
          vals_new <- vals[1,]
          vals_new$start <- min(vals$start)
          vals_new$end <- max(vals$end)
          vals_new$QA <- round(mean(as.numeric(vals$QA)))
          vals_new$QS <- max(vals$QS)
          vals_new$QSS <- vals$QSS[1]
          vals_new$QSE <- tail(vals$QSE, 1)
          vals_new$num_exon <- sum(vals$num_exon)
          vals_new$defragged <- TRUE
          return(vals_new)
        }))
      }
    }    ### Now return to genomic-coordinate space, and annotate for whether defragged
    if(length(ol_reinsert)>0){
      tab_out <- tab[-unique(subjectHits(ol_reinsert)),]
      tab_out <- rbind(tab_out, replacements)
      tab_out <- tab_out[order(tab_out$sample, tab_out$chr, tab_out$start),]
    }
    return(tab_out)
}

toBinSpace <- function(gr, bins, pattern="-"){
    tmp <- GRanges(str_replace(as.character(seqnames(gr)), paste0(".*", pattern), ""), IRanges(start(gr), end(gr)))
    ol <- findOverlaps(tmp, bins)    
    info <- matrix(unlist(by(subjectHits(ol), queryHits(ol), function(x) c(min(x), max(x)))), ncol=2, byrow=TRUE)
    strds <- strand(gr)[unique(queryHits(ol))]
    out <- GRanges(seqnames(gr)[unique(queryHits(ol))], IRanges(info[,1], info[,2]), strand=strds)
    values(out) <- values(gr)[unique(queryHits(ol)),]
    return(out)
}
createBins <- function(tab){
  return(GRanges(tab[,1], IRanges(tab[,2], tab[,3])))
}
divideMultiple <- function(svtk){
    svtk_multi <- svtk[str_detect(svtk$call_name, ","),]
    splits <- str_split(svtk_multi$call_name, ",")
    lens <- sapply(splits, length)
    expanded <- data.frame(name=rep(svtk_multi[,1], times=lens), call_name=unlist(splits))
    svtk <- svtk[-which(str_detect(svtk$call_name, ",")),]
    svtk <- rbind(svtk, expanded)
    return(svtk)
}

makeFiles <- function(name, cohort, mat, gbucket){
    output_name <- paste0(cohort, "-", name, ".txt")
    colind <- which(colnames(mat)==name)
    tmp <- mat[,colind]
    tmp <- str_replace_all(tmp, '\"', "")
    files <- unlist(str_split(str_replace_all(tmp, "(\\[)|]", ""), ","))
    write.table(files, quote=FALSE, sep="\t", col.names=F, row.names=F, file=paste0("~/downloads/", output_name))
    system2("gsutil", paste0("cp ~/downloads/", output_name, " ", gbucket, output_name))
    #file.remove(paste0("~/downloads/", output_name))
}

plot_ploidy <- function(df, i=1){
  ggplot(df, aes(x=START, y=LINEAR_COPY_RATIO)) + 
          geom_point(alpha=0.2) + 
          geom_hline(aes(yintercept=2)) +
          geom_hline(aes(yintercept=1)) +
          ylim(-0.5, 4) + 
          labs(x="Start Coordinate", 
               y="linear copy ratio", 
               title=sprintf("%s; %s; ploidy: %s", 
                             df$sample[1], 
                             df$CONTIG[i], 
                             ploidy_df[ploidy_df$CONTIG==df$CONTIG[i] & ploidy_df$sample==df$sample[1],]$PLOIDY[1]) %>% wrapper(30) )
}
```

#Step 1
Manally download the sample_set_entity.tsv file from firecloud. Set the wd (working directory) and jar_path variables to point to the working directory and the gCNV_helper.jar file. Best practices for directory structure:
  
  - Each time you download the zip file containing sample_set_entity.tsv and sample_set_membership.tsv, extract the zip file into a labeled directory, then change the wd to that directory. This will help with version control, file naming scheme, and pipeline reproducabiliy. In the eventuallity that you need to redo a step or trace previous steps, this is very helpful. 
  -You wil eventuly need to download the manifest file mulitple times and will end up having different manifest files with the same name. Here is what my project directory looks like
        gCNV_project_name/
        |-- sample_set_entity_barcode_2020_01_01/
        |   |-- countCramGrp_1/
        |   |   |-- sample_0001.barcode.counts.tsv
        |   |   |-- sample_0002.barcode.counts.tsv ...
        |   |-- countCramGrp_2/
        |   |   |-- sample_0501.barcode.counts.tsv
        |   |   |-- sample_0502.barcode.counts.tsv ...
        |   |-- countCramGrp_3/ ...
        |   |-- sample_set_entity.tsv
        |   |-- sample_set_membership.tsv
        |   |-- pca.rda
        |   |-- counts_matrix.tsv
        |
        |-- sample_set_entity_dataMunging_2020_01_07/
        |   |-- sample_set_entity.tsv
        |   |-- sample_set_membership.tsv
        |   |-- new_labels.tsv
        |
        |-- sample_set_entity_clustering_2020_01_14/
        |   |-- cluster_1_1_CASE/
        |   |   |-- cluster_1_1_CASE.java.bed
        |   |   |-- genotyped-segments-sample_0001.vcf
        |   |   |-- genotyped-segments-sample_0001.vcf.gz
        |   |   |-- genotyped-segments-sample_0002.vcf
        |   |   |-- genotyped-segments-sample_0002.vcf.gz ...
        |   |-- cluster_1_1_COHORT/
        |   |   |-- cluster_1_1_COHORT.java.bed
        |   |   |-- genotyped-segments-sample_0501.vcf
        |   |   |-- genotyped-segments-sample_0501.vcf.gz
        |   |   |-- genotyped-segments-sample_0502.vcf
        |   |   |-- genotyped-segments-sample_0502.vcf.gz ...
        |   |-- cluster_1_2_CASE/ ...
        |   |-- sample_set_entity.tsv
        |   |-- sample_set_membership.tsv
        |   |-- clustered.bed
        |   |-- gcnv_defragged.tsv
        |   |-- svtk_input.tsv
        |   |-- svtk_output.tsv
        |   |-- out.rda
        |   |-- final_callset_2020_01_14.tsv
        |
        |-- Plots
            |-- QC_plot_01.pdf
            |-- QC_plot_02.pdf
            |-- QC_plot_03.pdf
        
```{r}
wd <- "/path/to/folder/"
jar_path <- "/path/to/jar"
system2("java", sprintf("-Xmx16G -jar %s --help", jar_path)) # you might need to change -Xmx to how much memory you have available on your device
```


#Step 2
Download the barcode counts files to the wd. If the column name in your manifest file isn't "counts_barcode" then change code. This chunk just makes a system call to run the jar, which itself makes a system call to gsutil to download the files. It's turtles all the way down. 
```{r}
wd <- "/path/to/folder/"
system2("java", sprintf("-Xmx16G -jar %s getBarcodeCounts %ssample_set_entity.tsv %s counts_barcode", jar_path, wd, wd ))
```

#Step 3
Convert the barcode count files into a matrix file where rows are samples and columns are intervals
```{r}
wd <- "/path/to/folder/"
system2("java", sprintf("-Xmx16G -jar %s getCountsMatrix %s %scounts_matrix.tsv .barcode.counts.tsv", jar_path, wd, wd ))
```

#Step 4
- perform data normalization
- remove x and y chromosomes
- perform PCA
- save the R PCA object
a scree plot for the PCA loadings is generated to help visualize the data. 
```{r}
wd <- "/path/to/folder/"
counts_df <- read.table(paste0(wd, "counts_matrix.tsv"), sep="\t", stringsAsFactors = FALSE, header=TRUE, fill=TRUE)

counts_df[is.na(counts_df)] <- 0
counts_matrix <- t(as.matrix(counts_df))
indexes_to_remove <- rep(FALSE, nrow(counts_matrix))
rown <- rownames(counts_matrix)
chr <- do.call(rbind, str_split(rown, "_"))[,1]
bool_y <- chr == "X" | chr == "chrX" | chr == "ChrX"
bool_x <- chr == "Y" | chr == "chrY" | chr == "ChrY"
indexes_to_remove[bool_y | bool_x] <- TRUE
mydata_filtered <- counts_matrix[!indexes_to_remove,]
mydataNormalized <- t(t(mydata_filtered)/colSums(mydata_filtered, na.rm = TRUE))

pca <- prcomp(t(mydataNormalized), rank = 7)
fviz_eig(pca)
plot3d(pca$x[,c(1,2,3)])
save(pca, file=paste0(wd, "pca.rda"))
x <- pca$x
save(x, file=paste0(wd, "x.rda"))
```

#Step 5
- Pseudo-automate clustering. 
This step uses unsupervised clustering and metrics, so manual curration of final clustering is highly recomended. Typically, the best choice is among the top four recommended options. I am currently using only hclust. From previous experiences, it tends to work the best on average. Density based methods are currently in development. The rval data frame will store the results of each clustering metric. The final column is a ranking of all the metrics. By default, the first choice is chosen for clustering. However, you should manually check the data frame to see if any other combination has similar rankings in metrics to the top choice. The top choice is not necessarily the best. Change the 'n_clusters' variable based on the number of clusters you want. 
```{r}
wd <- "/path/to/folder/"
load(paste0(wd, "x.rda"))

set.seed(123)
pca_loadings <- x
n_clusters <- 25
hclust_methods <- c("ward.D", "ward.D2", "single", "complete", "average", "mcquitty", "median", "centroid") 
distance_methods <- c("euclidean", "maximum", "canberra", "minkowski")
mink_p <- c(10)
results <- expand.grid(hclust_methods, distance_methods)
colnames(results) <- c("agglomeration", "distance")
results$db <- 0
results$silhouette <- 0
results$dunn <- 0

dist_eucl <- dist(pca_loadings[,1:7], method="euclidean") 
dist_maxi< - dist(pca_loadings[,1:7], method="maximum") 
dist_canb <- dist(pca_loadings[,1:7], method="canberra") 
dist_mink <- dist(pca_loadings[,1:7], method="minkowski", p=2)
for(i in 1:nrow(results)){
  print(i)
  #dist_mat <- dist(pca_loadings[,1:7], method=results$distance[i], p=ifelse(results$distance[i]=="minkowski", mink_p, 2)) 
  dist_mat <- switch(results$distance[i], "euclidean"=dist_eucl, "maximum"=dist_maxi, "canberra"=dist_canb, "minkowski"=dist_mink)
  hclust <- hclust(dist_mat, method=results$agglomeration[i]) 
  cut_avg <- cutree(hclust, k=n_clusters)
  
  results$db[i] <- index.DB(pca_loadings[,1:3], cut_avg, centrotypes="centroids", p=3)$DB
  results$silhouette[i] <- mean(silhouette(cut_avg, dist_mat)[,3])
  results$dunn[i] <- dunn(dist_mat, cut_avg)
}

results <- results[order(results$silhouette, decreasing = TRUE),]
rval <- results
rval$db <- order(results$db, decreasing = FALSE)
rval$dunn <- order(results$dunn, decreasing = TRUE)
rval$silhouette <- order(results$silhouette, decreasing = TRUE)
rval$sum <- rval$db + rval$dunn + rval$silhouette
rval <- rval[order(rval$sum),]
write.table(rval , paste0(wd, "clustering_metrics", n_clusters, ".tsv"), sep="\t", quote = FALSE, col.names = TRUE, row.names = FALSE)
```

#Step 6
Plot the clustering output. Change the 'choice' variable from 1 to 33 to cycle through the different clustering methods. (lower is "better")
```{r}
n <- n_clusters
choice <- 1
dist_mat <- switch(results$distance[choice], "euclidean"=dist_eucl, "maximum"=dist_maxi, "canberra"=dist_canb, "minkowski"=dist_mink)
hclust <- hclust(dist_mat, method=rval$agglomeration[choice]) 
cut_avg <- cutree(hclust, k=n)
sorted_clusters <- sapply(unname(cut_avg), function(x) which(x == names(srt(cut_avg))))
cols <- rainbow(n)[sample(1:n, n)]
plot3d(x[,1:3], col=cols[sorted_clusters])
```

merge the small clusters to their nearest neighbor
```{r}
set.seed(123)
threshold <- 200
cluster_centers <- do.call(rbind, lapply(1:n_clusters, function(i){
  colMeans(x[sorted_clusters==i, ])
}) ) %>% as.data.frame()

tab <- table(sorted_clusters)
which_clusters_to_relabel <- which(tab < threshold) %>% unname
clusters_to_relabel <- x[sorted_clusters %in% which_clusters_to_relabel, ]
bool_cluster_relabel <- sorted_clusters %in% which_clusters_to_relabel
main_cluster_centers <- cluster_centers[-c(which_clusters_to_relabel),]


nearest_clusters <- lapply(1:nrow(x), function(i){
  if(!bool_cluster_relabel[i]){ 
    return(sorted_clusters[i])
  } else {
    dists <- as.matrix(dist(rbind(x[i, ], main_cluster_centers)))[1, -1]
    return(as.numeric(names(dists)[which(dists==min(dists))][1])-1)
  }
}) %>% unlist %>% as.numeric()

l1 <- length(unique(nearest_clusters))
cols <- rainbow(l1)[sample(1:l1, l1)]
plot3d(x[,1:3], col=cols[nearest_clusters])


cohort_size <- 200
memb <- do.call(rbind, lapply(sort(unique(nearest_clusters)), function(i){
  if(length(which(sorted_clusters==i)) <= cohort_size){
    df <- data.frame(membership=paste0("cluster_", i, "_COHORT"), 
                     samples = row.names(pca$x)[which(nearest_clusters==i)])
    return(df)
  } else {
    cohort_subset_indexes <- base::sample(which(sorted_clusters==i), cohort_size, replace = FALSE)
    nearest_points_in_cluster_indexes <- which(nearest_clusters==i)
    case_subset_indexes <- base::setdiff(nearest_points_in_cluster_indexes, cohort_subset_indexes)
    
    cohort_label <- paste0("cluster_", i, "_COHORT")
    case_label <- paste0("cluster_", i, "_CASE")
    
    cohort_samples <- row.names(pca$x)[cohort_subset_indexes]
    case_samples <- row.names(pca$x)[case_subset_indexes]
    
    df <- data.frame(membership=c(
      rep(cohort_label, length(cohort_samples)), 
      rep(case_label, length(case_samples)) ), 
      samples=c(
        cohort_samples,
        case_samples),
      stringsAsFactors = FALSE)
    return(df)
  }
}) )
write.table(memb, paste0(wd, "membership.tsv"), sep="\t", col.names = TRUE, row.names = FALSE, quote=FALSE)
```

You might need to re-match the ID's, you may need to change the regex arg of grepl()
```{r}
entity <- read.table(paste0(wd, "sample.tsv"), sep="\t", header = TRUE, stringsAsFactors = FALSE)
entity <- entity[grepl("cram", entity$cram_or_bam_path), ]
actual_names <- lapply(memb$samples, function(i) entity$entity.sample_id[grepl(i, entity$entity.sample_id)] )
table(sapply(actual_names, length) ) # IF THESE ARE NOT ALL '1' YOU HAVE A PROBLEM
df2 <- data.frame(cluster=memb$membership, sample=unlist(actual_names))
write.table(df2, paste0(wd, "CASE_COHORT_groups.tsv"), sep="\t", col.names =FALSE, quote=FALSE, row.names = FALSE)


df3 <- read.table(paste0(wd, "samples.tsv"), sep="\t", header=TRUE, stringsAsFactors = FALSE)
to_remove_samples <- df3$entity.sample_id[is.na(df3$exons_counts) & grepl("cram", df3$cram_or_bam_path)]
df4 <- df2[!df2$sample %in% to_remove_samples, ]
write.table(df4, paste0(wd, "CASE_COHORT_groups_trimmed.tsv"), sep="\t", col.names =FALSE, quote=FALSE, row.names = FALSE)
```

#Step 7
Save a 3d animation of the plot
3d rotate
```{r}
open3d()
plot3d(x[,1:3], col=cols[sorted_clusters])
if (!rgl.useNULL())
  play3d(spin3d(axis = c(1, 1, 1), rpm = 4), duration = 15, )
  movie3d( spin3d(rpm=3), duration=20,dir="C:/test/movie", clean=FALSE )
```

#Step 11
After running cohort mode, you will need to upload certain reference files so that case mode can use the model built by cohort mode
```{r}
wd <- "/path/to/folder/"
meta <- read.table(paste0(wd, "sample_set_entity.tsv"), sep="\t", header=TRUE, stringsAsFactors = FALSE)
cohorts <- meta[,1][grepl("COHORT", meta[,1])]  # change to your cohort
pse_case <- NULL
gbucket = 'gs://.....' # change to your gbucket
for(i in 1:length(cohorts)){
    cohort <- cohorts[i]
    ind <- which(meta[,1] == cohort)
    case <- str_replace(cohort, "COHORT_", "CASE_")
    if(case %in% meta[,1]){
        message(case)
        makeFiles("calling_configs", case, meta[ind,], gbucket)
        makeFiles("denoising_configs", case, meta[ind,], gbucket)
        makeFiles("gcnvkernel_version", case, meta[ind,], gbucket)
        makeFiles("gcnv_model_tars", case, meta[ind,], gbucket)
        makeFiles("sharded_interval_lists", case, meta[ind,], gbucket)    
        pse_case <- rbind(pse_case, meta[ind,])
    }
}
## Update the PARTICIPANT SET file
pse_case <- pse_case[, which(colnames(pse_case) %in% c("entity.sample_set_id", "intervals", "filtered_intervals", "contig_ploidy_model_tar"))]
    pse_case[,1] <- str_replace(pse_case[,1], "COHORT_", "CASE_")    
    pse_case$file_gcnv_model_tars <- paste0(gbucket, pse_case[,1], "-gcnv_model_tars.txt")
    pse_case$file_calling_configs <- paste0(gbucket, pse_case[,1], "-calling_configs.txt")
    pse_case$file_denoising_configs <- paste0(gbucket, pse_case[,1], "-denoising_configs.txt")
    pse_case$file_gcnvkernel_version <- paste0(gbucket, pse_case[,1], "-gcnvkernel_version.txt")
    pse_case$file_sharded_interval_lists <- paste0(gbucket, pse_case[,1], "-sharded_interval_lists.txt")        
        pse_case <- rbind(as.character(colnames(pse_case)), apply(pse_case, 2, as.character))
        pse_case[1,1] <- "entity:sample_set_id"
        write.table(pse_case, sep="\t", row.names=F, col.names=F, file=paste0("~/downloads/c_pse.txt"), quote=F)
```


#Step 12
on Terra/Fireloud, run gCNV. 

#Step 13
Download the segment VCFs and unzip them. 
```{r}
wd <- "/path/to/folder/"
system2("java", sprintf("-Xmx16G -jar %s downloadSegmentsVCFs %ssample_set_entity.tsv %s genotyped_segments_vcf", jar_path, wd, wd ))
```

#Step 14
Convert VCFs to BED format
```{r}
system2("java", sprintf("-Xmx16G -jar %s convertVCFsToBEDFormat %s svtk_input.bed genotyped-segments- .vcf", jar_path, wd, wd ))
```

#Step 15
Cluster calls together. You will need to install svtk: https://github.com/talkowski-lab/svtk
```{r}
system2("svtk", sprintf("bedcluster %ssvtk_input.bed %ssvtk_output.bed", wd, wd))
```

#Step 16
Merge the input and output of svtk, as svtk will delete exra metadata fields
```{r}
system2("java", sprintf("-Xmx16G -jar %s svtkMatch %ssvtk_input.bed %ssvtk_output.bed %sclustered.bed", jar_path, wd, wd, wd))
```

#Step 17
defragment calls. You will need your original intervals file
```{r}
wd <- "C:/Users/iwong/Documents/MGH/CMG_2020_03_04/2020_03_23_refresh/all_finished_runs/"
gcnv <- read.table(paste0(wd, "clustered.bed"), sep="\t", stringsAsFactors=FALSE, header=FALSE)
colnames(gcnv) <- c("chr", "start", "end", "name", "svtype", "sample", "call_name", "vaf", "vac", "pre_rmsstd", "post_rmsstd", "CN", "GT", "NP", "QA", "QS", "QSE", "QSS", "ploidy", "strand")
gcnv$batch <- str_extract(gcnv$call_name, "cluster.*[(CASE)|(COHORT)]")
gcnv$num_exon <- -1 # currently not supported
out_list <- vector("list", length = length(unique(gcnv$batch)))
fragment_threshold <- 20
ext <- .3

intervals <- read.table(paste0(wd, "aux_gencode_v33_split_800.bed"), comment.char="#")
bins_tab <- rep(list(intervals), length(unique(gcnv$batch)))
createBins <- function(tab){return(GRanges(tab[,1], IRanges(tab[,2], tab[,3])))}
list_bins <- lapply(bins_tab, createBins)
names(list_bins) <- unique(gcnv$batch)

for(i in 1:length(unique(gcnv$batch))){
  x <- unique(gcnv$batch)[i]
  message(x)
  gcnv_toDefrag <- gcnv[as.numeric(gcnv$QS)>=fragment_threshold & gcnv$batch==x,]
  gcnv_defrag <- deFragment(gcnv_toDefrag, list_bins[[x]], ext)    
  gcnv_notDefrag <- gcnv[as.numeric(gcnv$QS)<fragment_threshold & gcnv$batch==x,]
  gcnv_notDefrag$defragged <- FALSE
  gcnv_notDefrag$cprotect <- FALSE    
  gr_df <- GRanges(paste(gcnv_defrag$sample, "::::", gcnv_defrag$chr), IRanges(gcnv_defrag$start, gcnv_defrag$end))
  gr_ndf <- GRanges(paste(gcnv_notDefrag$sample, "::::", gcnv_notDefrag$chr), IRanges(gcnv_notDefrag$start, gcnv_notDefrag$end))
  suppressWarnings(ol <- GenomicRanges::findOverlaps(gr_df, gr_ndf)    )
  gcnv_defrag$mask_defrag <- FALSE
  gcnv_notDefrag$mask_defrag <- FALSE
  if(length(ol)>0){gcnv_notDefrag$mask_defrag[unique(subjectHits(ol))] <- TRUE}    
  out <- rbind(gcnv_defrag, gcnv_notDefrag)
  out <- out[order(out$sample, out$chr, out$start),]
  out_list[[i]] <- out
}
gcnv_defragged <- do.call(rbind, out_list)
gcnv_defragged <- gcnv_defragged[gcnv_defragged$mask_defrag==FALSE,]
gcnv_defragged$id <- 1:dim(gcnv_defragged)[1]

write.table(as(gcnv_defragged, "data.frame"), file = paste0(wd, "gcnv_defragged.tsv"), sep="\t", row.names = TRUE, col.names = TRUE, quote=FALSE)
```


You are now basically done. the next steps are generic filtering and metric steps. 

#Step 18 - QC and figures
label qs_median label. I'm doing this is in java because I'm not good enough with R to make a version that runs in a reasonable timeframe
Add PASS/FAIL metrics. TRUE=PASS    FALSE=FAIL
```{r}
system2("java", sprintf("-Xmx16G -jar %s labelMedianQS %sgcnv_defragged.tsv name QS %sgcnv_qsMed_intermediate.tsv", jar_path, wd, wd))
gcnv_final <- read.table(paste0(wd, "gcnv_qsMed_intermediate.tsv"), sep="\t", header=TRUE, stringsAsFactors = FALSE, fill=TRUE)

gcnv_final$PASS_QS <- (gcnv_final$svtype=="DUP" & gcnv_final$QS>=50) | (gcnv_final$svtype=="DEL" & gcnv_final$QS>=100) | (gcnv_final$CN==0 & gcnv_final$QS>=400)
gcnv_final$PASS_freq <- gcnv_final$vaf <= 0.01

lt100_raw_calls <- table(gcnv_final$sample)[which(table(gcnv_final$sample) <= 100)] %>% names
lt10_highQS_rare_calls <- table(gcnv_final$sample[gcnv_final$PASS_QS & gcnv_final$PASS_FREQ])[which(table(gcnv_final$sample[gcnv_final$PASS_QS & gcnv_final$PASS_FREQ]) <= 10)] %>% names
gcnv_final$PASS_sample <- (gcnv_final$sample %in% lt100_raw_calls) & ( (gcnv_final$sample %in% lt10_highQS_rare_calls))

# ver B - this takes an obscenely long time, R doesn't really support hashing so access times take forever
#t1 <- as.numeric(Sys.time())
#variant_calls <- gcnv_final$name %>% unique
#n_cores <- detectCores() - 1
#cl <- makeCluster(n_cores)
#clusterExport(cl=cl, varlist = c("gcnv_final", "variant_calls"))
#list_by_variant_calls <- parLapply(cl, seq_along(variant_calls), function(x) {
# df1 <- gcnv_final[gcnv_final$name==variant_calls[x], ]
# df1$QS_MEDIAN <- median(df1$QS, na.rm=TRUE)
# return(df1)
#})
#stopCluster(cl)
#t2 <- as.numeric(Sys.time())
#df_c <- do.call(rbind, list_by_variant_calls)

df_c <- gcnv_final
df_c$PASS_QS_var <- (df_c$svtype=="DUP" & df_c$QS_MEDIAN>=50) | (df_c$svtype=="DEL" & df_c$QS_MEDIAN>=100 & df_c$CN != 0) | (df_c$CN==0 & df_c$QS_MEDIAN>=400)

df_d <- df_c
df_d$site_frequency <- df_d$vaf
df_d$call_name <- NULL
df_d$vac <- NULL
df_d$vaf <- NULL
df_d$pre_rmsstd <- NULL
df_d$post_rmsstd <- NULL
df_d$num_exon <- NULL
df_d$cprotect <- NULL
df_d$mask_defrag <- NULL
df_d$id <- NULL

write.table(df_d, paste0(wd, "final_callset.tsv"), sep="\t", quote=FALSE, row.names = FALSE)
system2("java", sprintf("-Xmx16G -jar %s jointCallVCF %sfinal_callset.tsv %sjoint_callset.tsv name sample", jar_path, wd, wd))
```


look at aneuploidies. First need to download the ploidy information
Option A
```{r}
wd <- "C:/Users/iwong/Documents/MGH/CMG_2020_03_04/2020_03_23_refresh/downloads/"
wdu <- wd %>% str_replace_all("C:/", "/mnt/c/")

entity <- read.table(paste0(wd, "sample_set_entity.tsv"), sep="\t", quote="", stringsAsFactors=FALSE, header=TRUE)
contig_ploidy_calls <-c(entity$contig_ploidy_calls_tar, entity$contig_ploidy_calls_tars)
contig_ploidy_calls <- as.list(contig_ploidy_calls[contig_ploidy_calls!=""])
temp1 <- unlist(lapply(contig_ploidy_calls, function(x) x %>% str_replace_all("[\"|\\[|\\]]", "") %>% str_split(",")))
for(i in seq_along(temp1)){
  message(paste0("bash -c ", sprintf("'gsutil cp %s %sploidyTar%s/contig-ploidy.tar.gz'", temp1[i], wdu, i)))
  dir.create(sprintf("%sploidyTar%s/", wd, i))
  system2("bash", sprintf("-c 'gsutil cp %s %sploidyTar%s/contig-ploidy.tar.gz'", temp1[i], wdu, i))
}

system2("java", sprintf("-Xmx16G -jar %s downloadSegmentsVCFs %ssample_set_entity.tsv %s contig_ploidy_calls_tars", jar_path, wd, wd ))
system2("java", sprintf("-Xmx16G -jar %s downloadSegmentsVCFs %ssample_set_entity.tsv %s contig_ploidy_calls_tar", jar_path, wd, wd ))
```

Option B
input is manifest file
#!/bin/bash

wcl=$(wc -l ${1} | cut -d " " -f1)
echo $wcl


for i in $(seq 1 ${wcl})
do
        file=$(sed ''${i}'!d' ${1})
        echo ${file}
        mkdir "ploidy_file_${i}"
        gsutil cp ${file} "./ploidy_file_${i}/ploidy_file_${i}.tar.gz"
done


#!/bin/bash
for d in $(ls -d ploidy*/)
do
echo $d
cd $d
tar -xf *tar.gz
cd ..
done

Get ploidy conunts
```{r}
library(stringr)
wd <- "C:/Users/iwong/Documents/"
ploidy_files <- list.files(paste0(wd ,""), pattern="contig_ploidy.tsv", recursive = TRUE, full.names = TRUE)

ploidy_dfs <- lapply(ploidy_files, function(x){
  df <- read.table(x, comment.char = "@", header=TRUE, stringsAsFactors=FALSE, quote="", fill=TRUE)
  df$sample <- str_extract_all(scan(x, what="character", sep="\n", nlines=1, quiet=TRUE)[[1]], "(?<=SM:).*")[[1]]
  return(df)
})

ploidy_df <- do.call(rbind, ploidy_dfs)
sex_ploidy_sums <- do.call(c, lapply(ploidy_dfs, function(df) sum(df$PLOIDY[str_detect(df$CONTIG, "X|Y")])))
sex_ploidy_samples <- unique(do.call(rbind, ploidy_dfs[which(sex_ploidy_sums!=2)])$sample)
autosome_ploidy_samples <- ploidy_df[ploidy_df$PLOIDY!=2 & !str_detect(ploidy_df$CONTIG, "X|Y"),]$sample
ploidy_samples <- c(sex_ploidy_samples, autosome_ploidy_samples)
```

Download DCR files
```{r}
# This will take a while, you might want to subset to files with aneuploidies
system2("java", sprintf("-Xmx16G -jar %s getBarcodeCounts %ssample_set_entity.tsv %s denoised_copy_ratios", jar_path, wd, wd ))
```


Plot ploidy counts,
```{r}
wd <- "C:/Users/iwong/Documents/MGH/"
dcr_files <- list.files(wd, pattern="denoised_copy_ratios-", recursive = TRUE, full.names = TRUE)
temp1 <- str_detect(dcr_files, paste0("(",paste(ploidy_samples, collapse=")|(") , ")"))
temp2 <- dcr_files[temp1]

cluster <- makeCluster(detectCores()-1)
clusterExport(cl=cluster, varlist=c("temp2", "str_detect", "str_extract"))
dcr_dfs <- parLapply(cluster, temp2, function(x){
  lines <- scan(x, what="character", sep="\n", quiet=TRUE)
  df <- read.table(textConnection(lines), comment.char = "@", header=TRUE, stringsAsFactors=FALSE, quote="", fill=TRUE)
  df$sample <- str_extract(lines[str_detect(lines, "SM:")], "(?<=SM:).*")[[1]]
  return(df)
})
stopCluster(cluster)
dcr_df <- do.call(rbind, dcr_dfs)

#plot linear copy ratio over chromosome for autosomes
autosome_ploidy_dfs <- ploidy_df[ploidy_df$PLOIDY!=2 & (ploidy_df$sample %in% autosome_ploidy_samples) & !str_detect(ploidy_df$CONTIG, "X|Y"), ]
for(i in 1:nrow(autosome_ploidy_dfs)){
  df <- dcr_df[dcr_df$sample==autosome_ploidy_dfs$sample[i] & dcr_df$CONTIG==autosome_ploidy_dfs$CONTIG[i], ]
  print(plot_ploidy(df))
  pdf(sprintf("%sploidy_sample_%s_%s.pdf", wd, df$sample[1], df$CONTIG[1]))
  my_plot <- plot_ploidy(df)
  print(my_plot)
  dev.off()
}

sex_ploidy_dfs <- ploidy_df[(ploidy_df$sample %in% sex_ploidy_samples) & str_detect(ploidy_df$CONTIG, "X|Y"), ]
for(i in seq_along(unique(sex_ploidy_dfs$sample))){
  par(mfrow=c(1,2))
  df <- dcr_df[dcr_df$sample==unique(sex_ploidy_dfs$sample)[i], ]
  plot_x <- plot_ploidy(df[str_detect(df$CONTIG, "X"),])
  plot_y <- plot_ploidy(df[str_detect(df$CONTIG, "Y"),])
  grid.arrange(plot_x, plot_y, ncol=2)
  
  
  my_plot <- arrangeGrob(plot_x, plot_y, ncol=2)
  ggsave(sprintf("%sploidy_sample_%s_sex_chroms.pdf", wd, df$sample[1]), my_plot)
}
```



Main Figures
variant count boxplots
```{r}
library(wesanderson)
wd <- "C:/Users/iwong/Documents/"
gcnv <- read(wd, "final_callset.tsv")

qs_window <- seq(0, 100, 10)
total_variants <- rep(0, length(qs_window))
variant_counts <- vector("list", length(qs_window))

for(i in seq_along(qs_window)){
  filtered_qs <- gcnv$QS >= qs_window[i]
  filtered_vafs <- gcnv$site_frequency <= 0.01
  filtered <- gcnv[filtered_qs & filtered_vafs ,]
  total_variants[i] <- length(unique(filtered$name))
  variant_counts[[i]] <- as.numeric(table(filtered$sample))
}

n_outliers <- lapply(variant_counts, function(x) length(boxplot(x, plot=FALSE)$out) )

lengths <- sapply(variant_counts, length)
labels <- lapply(seq_along(qs_window), function(x){rep(qs_window[x], lengths[x])}) %>% unlist %>% as.character()

temp1 <- letters[seq_along(qs_window)]
labels2 <- lapply(seq_along(temp1), function(x){rep(temp1[x], lengths[x])}) %>% unlist
df <- data.frame(qs_threshold=labels, n_variants=unlist(variant_counts), class=labels2)
df$qs_threshold <- factor(df$qs_threshold, levels=qs_window)

df$n_variants_log2 <- (df$n_variants) %>% log2
df$n_variants_log10 <- df$n_variants %>% log10

gcnvhq <- gcnv[gcnv$PASS_freq & gcnv$PASS_QS & gcnv$PASS_sample, ]
#write.table(gcnvhq, paste0(wd, "gcnv_HQ.tsv"), sep="\t", row.names = FALSE, col.names = TRUE, quote=FALSE)
```


```{r}
g1 <- ggplot(df, aes(x=qs_threshold, y=n_variants)) +
  geom_boxplot(outlier.size = NA, outlier.shape = NA, color=wes_palette("Zissou1")[1], fill=wes_palette("Zissou1")[2], alpha=0.6) + 
  coord_cartesian(ylim = c(0, 80)) +
  xlab("QS threshold") + 
  ylab("# of Variants per sample") +
  ggtitle("Variants per sample, outliers not shown") +
  theme(plot.title = element_text(hjust = 0.5))
g1
```


```{r}
g2 <- ggplot(df[df$qs_threshold!=0 & df$qs_threshold!=10,], aes(x=qs_threshold, y=n_variants)) +
  geom_boxplot(outlier.size = NA, outlier.shape = NA, color=wes_palette("Zissou1")[1], fill=wes_palette("Zissou1")[2], alpha=0.6) + 
  coord_cartesian(ylim = c(0, 16)) +
  xlab("QS threshold") + 
  ylab("# of Variants per sample") +
  ggtitle("Variants per sample, outliers not shown") +
  theme(plot.title = element_text(hjust = 0.5))
g2
```

variant count histograms
```{r}
g3 <- ggplot(df[df$qs_threshold==0,] , aes(x=n_variants_log2)) +
  geom_histogram(aes(y=..density..), colour="BLACK", fill=wes_palette("FantasticFox1")[1]) + 
  geom_density(alpha=.2, bw=0.3, size=1, color=wes_palette("FantasticFox1")[5]) +
  ylab("frequency") + 
  xlab("number of variants") +
  ggtitle(bquote(~log[2]~"transformed number of variants; QS threshold=0 ")) +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_x_continuous(breaks=0:10, labels=2^(0:10))
g3
```

```{r}
bks <- seq(0, 10, 0.6)
g4 <- ggplot(df[df$qs_threshold==50,] , aes(x=n_variants_log2)) +
  geom_histogram(aes(y=..density..), colour="BLACK", fill=wes_palette("FantasticFox1")[1], breaks=bks) + 
  geom_density(alpha=.2, bw=0.5, size=1, color=wes_palette("FantasticFox1")[5]) +
  ylab("frequency") + 
  xlab("number of variants") +
  ggtitle(bquote(~log[2]~"transformed number of variants; QS threshold=50 ")) +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_x_continuous(breaks=0:10, labels=2^(0:10))
g4
```


```{r}
bks <- seq(0, 10, 0.6)
g5 <- ggplot(df[df$qs_threshold==100,] , aes(x=n_variants_log2)) +
  geom_histogram(aes(y=..density..), colour="BLACK", fill=wes_palette("FantasticFox1")[1], breaks=bks) + 
  geom_density(alpha=.2, bw=0.9, size=1, color=wes_palette("FantasticFox1")[5]) +
  ylab("frequency") + 
  xlab("number of variants") +
  ggtitle(bquote(~log[2]~"transformed number of variants; QS threshold=100 ")) +
  theme(plot.title = element_text(hjust = 0.5)) +
  scale_x_continuous(breaks=0:10, labels=2^(0:10))
g5
```


get counts of CNVS per bin
bedtools intersect -wa -a gencode_v33_split.bed -b gcnv_final.tsv |  awk '{print $1 "-" $2}' > intersections.bed
```{r}
if(FALSE){
  del_intersections <- read.table(paste0(wd, "del_intersection.tsv"), sep="\t", header=FALSE, stringsAsFactors = FALSE)
  dup_intersections <- read.table(paste0(wd, "dup_intersection.tsv"), sep="\t", header=FALSE, stringsAsFactors = FALSE)
  var_type <- "raw"
  
  count_samples_per_cnv <- table(gcnv$name)
  df_count_samples_per_cnv <- data.frame(name=names(count_samples_per_cnv), sample_Count= as.numeric(count_samples_per_cnv))
  variant_df <- gcnv[match(unique(gcnv$name), gcnv$name), c(1,2,4,5, 18, 19, 20)] %>% merge(df_count_samples_per_cnv)
  variant_df$bp <- variant_df$start
  variant_df$start <- NULL
  variant_df_del <- variant_df[variant_df$svtype=="DEL", ]
  variant_df_dup <- variant_df[variant_df$svtype=="DUP", ]
} else {
  del_intersections <- read.table(paste0(wd, "hq_del_intersections.tsv"), sep="\t", header=FALSE, stringsAsFactors = FALSE)
  dup_intersections <- read.table(paste0(wd, "hq_dup_intersections.tsv"), sep="\t", header=FALSE, stringsAsFactors = FALSE)
  var_type <- "HQ"
  
  gcnvhq <- gcnv[gcnv$PASS_freq & gcnv$PASS_QS & gcnv$PASS_sample, ]
  count_samples_per_cnv <- table(gcnvhq$name)
  df_count_samples_per_cnv <- data.frame(name=names(count_samples_per_cnv), sample_Count= as.numeric(count_samples_per_cnv))
  variant_df <- gcnvhq[match(unique(gcnvhq$name), gcnvhq$name), c(1,2,4,5, 18, 19, 20)] %>% merge(df_count_samples_per_cnv)
  variant_df$bp <- variant_df$start
  variant_df$start <- NULL
  variant_df_del <- variant_df[variant_df$svtype=="DEL", ]
  variant_df_dup <- variant_df[variant_df$svtype=="DUP", ]
}

del_freq <- table(del_intersections$V1)
del_df <- data.frame(name=names(del_freq), count=as.numeric(del_freq))
del_df$chr <- del_df$name %>% str_extract(".*(?=-)") 
del_df$bp <- del_df$name %>% str_extract_all("(?<=-).*") %>% as.numeric()

dup_freq <- table(dup_intersections$V1)
dup_df <- data.frame(name=names(dup_freq), count=as.numeric(dup_freq))
dup_df$chr <- dup_df$name %>% str_extract(".*(?=-)")
dup_df$bp <- dup_df$name %>% str_extract_all("(?<=-).*") %>% as.numeric()

```

#percent of cnvs per bin, for each bin, what percentage of cnv calls overlap it?
```{r}
out_dir <- paste0("CNV_per_bin_percent_local-scale_", var_type, "/")
dir.create(paste0(wd, out_dir))
lapply(unique(del_df$chr), function(x){
  temp_df_del <- del_df[del_df$chr==x,]
  temp_df_dup <- dup_df[dup_df$chr==x,]
  sum_del <- sum(temp_df_del$count)
  sum_dup <- sum(temp_df_dup$count)
  sum_all <- sum_del + sum_dup
  temp_df_del$percent_count <- temp_df_del$count / sum_all * 100
  temp_df_dup$percent_count <- temp_df_dup$count / sum_all * 100
  ymax=max(c(max(temp_df_del$percent_count, max(temp_df_dup$percent_count)))) * 1.05
  
  g_del <- ggplot(temp_df_del, aes(x=bp, y=percent_count)) +
    geom_point(colour=wes_palette("FantasticFox1")[5]) +
    theme(legend.position="none", plot.title = element_text(hjust = 0.5)) +
    xlab("coordinates") +
    ggtitle(paste0(x, " DEL")) + 
    ylim(0, ymax) + 
    ylab(paste0("percent of ", var_type," CNVs per bin, ", x))
  
  g_dup <- ggplot(temp_df_dup, aes(x=bp, y=percent_count)) +
    geom_point(colour=wes_palette("FantasticFox1")[3]) +
    theme(legend.position="none", plot.title = element_text(hjust = 0.5)
          , axis.title.y=element_blank(),) +
    xlab("coordinates") +
    ylim(0, ymax) + 
    ggtitle(paste0(x, " DUP"))
  
  png(paste0(wd, out_dir, "CNV_per_bin_percent_local-scale_", x, "_", var_type,".png"), width=14, height=7, units="in", res=300)
  grid.arrange(g_del, g_dup, ncol=2)
  dev.off()
})
```

#GLOBAL
```{r}
out_dir <- paste0("CNV_per_bin_count_global-scale_", var_type,"/")
dir.create(paste0(wd, out_dir))
ymax=max(c(max(del_df$count, max(del_df$count)))) * 1.05
lapply(unique(del_df$chr), function(x){
  temp_df_del <- del_df[del_df$chr==x,]
  temp_df_dup <- dup_df[dup_df$chr==x,]
  
  g_del <- ggplot(temp_df_del, aes(x=bp, y=count)) +
    geom_point(colour=wes_palette("FantasticFox1")[5]) +
    theme(legend.position="none", plot.title = element_text(hjust = 0.5)) +
    xlab("coordinates") +
    ylim(0, ymax) + 
    ggtitle(paste0(x, " DEL")) + 
    ylab(paste0("count of ", var_type," CNVs per bin, ", x))
  
  g_dup <- ggplot(temp_df_dup, aes(x=bp, y=count)) +
    geom_point(colour=wes_palette("FantasticFox1")[3]) +
    theme(legend.position="none", plot.title = element_text(hjust = 0.5)
          , axis.title.y=element_blank(),) +
    xlab("coordinates") +
    ylim(0, ymax) + 
    ggtitle(paste0(x, " DUP"))
  
  png(paste0(wd, out_dir, "CNV_per_bin_count_global-scale_", x, "_", var_type,".png"), width=14, height=7, units="in", res=300)
  grid.arrange(g_del, g_dup, ncol=2)
  dev.off()
})
```

#LOCAL
```{r}
out_dir <- paste0("CNV_per_bin_count_local-scale_", var_type, "/")
dir.create(paste0(wd, out_dir))
lapply(unique(del_df$chr), function(x){
  temp_df_del <- del_df[del_df$chr==x,]
  temp_df_dup <- dup_df[dup_df$chr==x,]
  ymax=max(c(max(temp_df_del$count, max(temp_df_dup$count)))) * 1.05
  
  g_del <- ggplot(temp_df_del, aes(x=bp, y=count)) +
    geom_point(colour=wes_palette("FantasticFox1")[5]) +
    theme(legend.position="none", plot.title = element_text(hjust = 0.5)) +
    xlab("coordinates") +
    ylim(0, ymax) + 
    ggtitle(paste0(x, " DEL")) + 
    ylab(paste0("count of ", var_type," CNVs per bin, ", x))
  
  g_dup <- ggplot(temp_df_dup, aes(x=bp, y=count)) +
    geom_point(colour=wes_palette("FantasticFox1")[3]) +
    theme(legend.position="none", plot.title = element_text(hjust = 0.5)
          , axis.title.y=element_blank(),) +
    xlab("coordinates") +
    ylim(0, ymax) + 
    ggtitle(paste0(x, " DUP"))
  
  png(paste0(wd, out_dir, "CNV_per_bin_count_local-scale_", x, "_", var_type,".png"), width=14, height=7, units="in", res=300)
  grid.arrange(g_del, g_dup, ncol=2)
  dev.off()
})
```

#percent of samples per bin, for each bin, what percentage of samples have a CNV call overlapping it?
```{r}
out_dir <- paste0("sample_per_bin_percent_local-scale_", var_type,"/")
dir.create(paste0(wd, out_dir))
lapply(unique(del_df$chr), function(x){
  temp_df_del <- variant_df_del[variant_df_del$chr==x,]
  temp_df_dup <- variant_df_dup[variant_df_dup$chr==x,]
  sum_del <- sum(temp_df_del$sample_Count)
  sum_dup <- sum(temp_df_dup$sample_Count)
  sum_all <- sum_del + sum_dup
  temp_df_del$percent_count <- temp_df_del$sample_Count / sum_all * 100
  temp_df_dup$percent_count <- temp_df_dup$sample_Count / sum_all * 100
  ymax=max(c(max(temp_df_del$percent_count, max(temp_df_dup$percent_count)))) * 1.05
  
  #print(paste(x, dim(temp_df_del), dim(temp_df_dup)))
  
  g_del <- ggplot(temp_df_del, aes(x=bp, y=percent_count)) +
    geom_point(colour=wes_palette("FantasticFox1")[5]) +
    theme(legend.position="none", plot.title = element_text(hjust = 0.5)) +
    xlab("coordinates") +
    ggtitle(paste0(x, " DEL")) + 
    ylim(0, ymax) + 
    ylab(paste0("percent of samples with ", var_type," CNVs per bin, ", x))
  
  g_dup <- ggplot(temp_df_dup, aes(x=bp, y=percent_count)) +
    geom_point(colour=wes_palette("FantasticFox1")[3]) +
    theme(legend.position="none", plot.title = element_text(hjust = 0.5)
          , axis.title.y=element_blank(),) +
    xlab("coordinates") +
    ylim(0, ymax) + 
    ggtitle(paste0(x, " DUP"))
  
  png(paste0(wd, out_dir, "percent_samples_per_bin_percent_local-scale_", x, "_", var_type,".png"), width=14, height=7, units="in", res=300)
  grid.arrange(g_del, g_dup, ncol=2)
  dev.off()
})
```




```{r}
jar_path <- "C:/Users/iwong/Documents/MGH/scripts/gCNV_helper.jar"
system2("java", sprintf("-Xmx16G -jar %s --help", jar_path))
system2("java", sprintf("-Xmx16G -jar %s getPerSampleMetrics  %sfinal_callset.tsv name %sby_variant.tsv", jar_path, wd, wd ))
system2("java", sprintf("-Xmx16G -jar %s getPerSampleMetrics  %sgcnv_HQ.tsv name %sby_HQ_variant.tsv", jar_path, wd, wd ))
system2("java", sprintf("-Xmx16G -jar %s getPerSampleMetrics  %sgcnv_HQ.tsv sample %sby_HQ_sample.tsv", jar_path, wd, wd ))
```

```{r}
variant_df <- read.table(paste0(wd, "by_HQ_sample.tsv"), sep="\t", header=TRUE, stringsAsFactors = FALSE, fill=TRUE)
variant_df$X <- NULL

df1 <- gcnv[match(unique(gcnv$name), gcnv$name),c(1,2,4,5)] %>% merge(variant_df)
df1 <- gcnv[match(unique(gcnv$sample), gcnv$sample),c(1,2,4,5)] %>% merge(variant_df)

df1$bp <- df1$start
dup_df <- df1[df1$svtype=="DUP", ]
del_df <- df1[df1$svtype=="DEL", ]
```


```{r}
column_to_plot <- "QS_MEDIAN"
lapply(unique(del_df$chr), function(x){
  temp_df_del <- del_df[del_df$chr==x,]
  temp_df_dup <- dup_df[dup_df$chr==x,]
  ymax=max(c(max(temp_df_del[column_to_plot], max(temp_df_dup[column_to_plot])))) * 1.05
  
  g_del <- ggplot(temp_df_del, aes(x=bp, y=QS_MEDIAN)) +
    geom_point(colour=wes_palette("FantasticFox1")[5]) +
    theme(legend.position="none", plot.title = element_text(hjust = 0.5)) +
    xlab("coordinates") +
    ylim(0, ymax) + 
    ggtitle(paste0(x, " DEL")) + 
    scale_y_continuous(trans = 'log2')    
  
  g_dup <- ggplot(temp_df_del, aes(x=bp, y=QS_MEDIAN)) +
    geom_point(colour=wes_palette("FantasticFox1")[3]) +
    theme(legend.position="none", plot.title = element_text(hjust = 0.5)
          , axis.title.y=element_blank(),) +
    xlab("coordinates") +
    ylim(0, ymax) + 
    ggtitle(paste0(x, " DUP")) +
    scale_y_continuous(trans = 'log2')  
  
  png(paste0(wd, column_to_plot, "_local-scale_", x, "_HQ-variants.png"), width=14, height=7, units="in", res=300)
  grid.arrange(g_del, g_dup, ncol=2)
  dev.off()
})
```

```{r}
column_to_plot <- "QS_MEAN"
lapply(unique(del_df$chr), function(x){
  temp_df_del <- del_df[del_df$chr==x,]
  temp_df_dup <- dup_df[dup_df$chr==x,]
  ymax=max(c(max(temp_df_del[column_to_plot], max(temp_df_dup[column_to_plot])))) * 1.05
  
  g_del <- ggplot(temp_df_del, aes(x=bp, y=QS_MEAN)) +
    geom_point(colour=wes_palette("FantasticFox1")[5]) +
    theme(legend.position="none", plot.title = element_text(hjust = 0.5)) +
    xlab("coordinates") +
    ylim(0, ymax) + 
    ggtitle(paste0(x, " DEL")) + 
    scale_y_continuous(trans = 'log2')    
  
  g_dup <- ggplot(temp_df_del, aes(x=bp, y=QS_MEAN)) +
    geom_point(colour=wes_palette("FantasticFox1")[3]) +
    theme(legend.position="none", plot.title = element_text(hjust = 0.5)
          , axis.title.y=element_blank(),) +
    xlab("coordinates") +
    ylim(0, ymax) + 
    ggtitle(paste0(x, " DUP")) +
    scale_y_continuous(trans = 'log2')  
  
  png(paste0(wd, column_to_plot, "_local-scale_", x, "_HQ-variants.png"), width=14, height=7, units="in", res=300)
  grid.arrange(g_del, g_dup, ncol=2)
  dev.off()
})
```


number of variants vs number of exons
bedtools intersect -c -a gcnv.tsv -b gencode.bed > gcnv_with_count.tsv
```{r}
#dup_count <- read.table(paste0(wd, "gcnv_with_count_dup.tsv"), sep="\t", header=FALSE, stringsAsFactors = FALSE)
#del_count <- read.table(paste0(wd, "gcnv_with_count_del.tsv"), sep="\t", header=FALSE, stringsAsFactors = FALSE)

dup_count <- read.table(paste0(wd, "gcnv_count_HQ_dup.tsv"), sep="\t", header=FALSE, stringsAsFactors = FALSE)
del_count <- read.table(paste0(wd, "gcnv_count_HQ_del.tsv"), sep="\t", header=FALSE, stringsAsFactors = FALSE)
colnames(dup_count) <- c(colnames(gcnv), "n_exons")
colnames(del_count) <- c(colnames(gcnv), "n_exons")

#gcnv2 <- gcnv2[gcnv2$PASS_freq==TRUE & gcnv2$PASS_QS==TRUE & gcnv2$PASS_sample==TRUE, ]
#gcnv3 <- gcnv3[gcnv3$PASS_freq==TRUE & gcnv3$PASS_QS==TRUE & gcnv3$PASS_sample==TRUE, ]

unique_dup <- dup_count[match(unique(dup_count$name), dup_count$name), ]
unique_del <- del_count[match(unique(del_count$name), del_count$name), ]

exon_table_dup <- table(unique_dup$n_exons)
exon_table_del <- table(unique_del$n_exons)

df_dup <- data.frame(nexons=names(exon_table_dup) %>% as.numeric(), nvariants=as.numeric(exon_table_dup))
df_dup <- df_dup[order(df_dup$nexons, decreasing = FALSE),]
for(i in seq(nrow(df_dup)-1, 1, -1)){
  df_dup$nvariants[i] = df_dup$nvariants[i] + df_dup$nvariants[i+1]
}

df_del <- data.frame(nexons=names(exon_table_del) %>% as.numeric(), nvariants=as.numeric(exon_table_del))
df_del <- df_del[order(df_del$nexons, decreasing = FALSE),]
for(i in seq(nrow(df_del)-1, 1, -1)){
  df_del$nvariants[i] = df_del$nvariants[i] + df_del$nvariants[i+1]
}

df_del$type <- "DEL"
df_dup$type <- "DUP"
df <- rbind(df_del, df_dup)
df$log2_nvars <- log2(df$nvariants)
df$log2_nexons <- log2(df$nexons)
```


```{r}
g6 <- ggplot(data=df[df$nexons<5000, ], aes(x=log2_nexons, y=nvariants, group=type)) +
  geom_line(aes(colour=type), size=1.5) +
  scale_color_manual(values=c(wes_palette("FantasticFox1")[5], wes_palette("FantasticFox1")[3])) +
  theme(legend.position="bottom") + 
  scale_x_continuous(breaks=0:12, labels=2^(0:12)) +
  xlab("number of exons") + 
  ylab("number of variants") + 
  ggtitle("Distribution of raw CNVs")
g6
```

proprtion of singletons
```{r}
singletons_del <- which(table(del_count$name[!del_count$sample %in% ploidy_samples])==1) %>% names
df_single_del <- del_count[del_count$name %in% singletons_del, ]
exon_table_single_del <- table(df_single_del$n_exons)
exon_table_del <- table(del_count$n_exons)
df_del <- data.frame(nexons=names(exon_table_del) %>% as.numeric(), nvariants=as.numeric(exon_table_del))
df_del <- df_del[order(df_del$nexons, decreasing = FALSE),]
df1_del <- data.frame(nexons=names(exon_table_single_del) %>% as.numeric(), nvariants_single=as.numeric(exon_table_single_del))
df1_del <- df1_del[order(df1_del$nexons, decreasing = FALSE),]
df2_del <- merge(df_del, df1_del)
for(i in seq(nrow(df2_del)-1, 1, -1)){
  df2_del$nvariants[i] = df2_del$nvariants[i] + df2_del$nvariants[i+1]
  df2_del$nvariants_single[i] = df2_del$nvariants_single[i] + df2_del$nvariants_single[i+1]
}
df2_del$proportion <- df2_del$nvariants_single/df2_del$nvariants*100
df2_del$type <- "DEL"

singletons_dup <- which(table(dup_count$name[!dup_count$sample %in% ploidy_samples])==1) %>% names
df_single_dup <- dup_count[dup_count$name %in% singletons_dup, ]
exon_table_single_dup <- table(df_single_dup$n_exons)
exon_table_dup <- table(dup_count$n_exons)
df_dup <- data.frame(nexons=names(exon_table_dup) %>% as.numeric(), nvariants=as.numeric(exon_table_dup))
df_dup <- df_dup[order(df_dup$nexons, decreasing = FALSE),]
df1_dup <- data.frame(nexons=names(exon_table_single_dup) %>% as.numeric(), nvariants_single=as.numeric(exon_table_single_dup))
df1_dup <- df1_dup[order(df1_dup$nexons, decreasing = FALSE),]
df2_dup <- merge(df_dup, df1_dup)
for(i in seq(nrow(df2_dup)-1, 1, -1)){
  df2_dup$nvariants[i] = df2_dup$nvariants[i] + df2_dup$nvariants[i+1]
  df2_dup$nvariants_single[i] = df2_dup$nvariants_single[i] + df2_dup$nvariants_single[i+1]
}
df2_dup$proportion <- df2_dup$nvariants_single/df2_dup$nvariants*100
df2_dup$type <- "DUP"

df3 <- rbind(df2_del, df2_dup)
df3$log2_nexons <- df3$nexons %>% log2
```


```{r}
g7 <- ggplot(data=df3[df3$nexons<5000  , ], aes(x=log2_nexons, y=proportion, group=type)) +
  geom_line(aes(colour=type), size=1.5) +
  scale_color_manual(values=c(wes_palette("FantasticFox1")[5], wes_palette("FantasticFox1")[3])) +
  theme(legend.position="bottom") + 
  scale_x_continuous(breaks=0:12, labels=2^(0:12)) +
  xlab("number of exons") + 
  ylab("percent of singletons") + 
  ggtitle("Distribution of HQ CNVs") + 
  ylim(0, 100) 
g7
```




overlapping histogram of size
```{r}
df <- gcnv[, c("start", "end", "svtype", "PASS_freq", "PASS_sample", "PASS_QS")]
df$size <- (df$end - df$start)/1000
df$size_log10 <- df$size %>% log10
df$size_log2 <- df$size %>% log2
df$HQ <- df$PASS_freq & df$PASS_QS & df$PASS_sample
```


```{r}
g8 <- ggplot(data=df[df$size>0.125 & df$size<1500, ], aes(x=size_log2, fill=svtype)) +
  geom_histogram(alpha=0.7, position = "identity", aes(y = ..density..)) +
  scale_fill_manual(values=c(wes_palette("FantasticFox1")[5], wes_palette("FantasticFox1")[3])) +
  scale_x_continuous(breaks=-4:10, labels = 2^(-4:10)) + 
  xlab("CNV size (KB)") + 
  ggtitle("Raw CNV size distribution") 
g8
```

```{r}
g9 <- ggplot(data=df[df$size>0.125 & df$size<1500 & df$HQ, ], aes(x=size_log2, fill=svtype)) +
  geom_histogram(alpha=0.7, position = "identity", aes(y = ..density..)) +
  scale_fill_manual(values=c(wes_palette("FantasticFox1")[5], wes_palette("FantasticFox1")[3])) +
  scale_x_continuous(breaks=-4:10, labels = 2^(-4:10)) + 
  xlab("CNV size (KB)") + 
  ggtitle("HQ CNV size distribution")
g9
```

venn diagram

```{r}
wd <- "C:/Users/iwong/Documents/"
gcnv <- read(wd, "final_callset.tsv")

samp <- c(TRUE, FALSE)
freq <- c(TRUE, FALSE)
qs <- c(TRUE, FALSE)
temp_df <- expand.grid(samp, freq, qs)
colnames(temp_df) <- c("samp", "freq","qs")
temp_df$count <- -1
temp_df$percent_of_all_calls <- -1
temp_df$per_sample <- -1
for(i in 1:nrow(temp_df)){
  temp_df$count[i] <- nrow(gcnv[gcnv$PASS_sample==temp_df$samp[i] & gcnv$PASS_freq==temp_df$freq[i] & gcnv$PASS_QS==temp_df$qs[i],])
  temp_df$perc[i] <- temp_df$count[i] / nrow(gcnv) * 100
  temp_df$per_sample[i] <- temp_df$count[i] / (gcnv$sample %>% unique %>% length)
}


length(unique(gcnv[gcnv$PASS_sample==temp_df$samp[i] & gcnv$PASS_freq==temp_df$freq[i] & gcnv$PASS_QS==temp_df$qs[i],]$sample))
dim(gcnv[gcnv$PASS_sample==temp_df$samp[i] & gcnv$PASS_freq==temp_df$freq[i] & gcnv$PASS_QS==temp_df$qs[i],])
length(unique(gcnv[gcnv$PASS_sample==temp_df$samp[i] & gcnv$PASS_freq==temp_df$freq[i] & gcnv$PASS_QS==temp_df$qs[i],]$name))

length(unique(gcnv$sample))
dim(gcnv)
length(unique(gcnv$name))

temp_df

pass_qs <- paste0(gcnv$sample[gcnv$PASS_QS], gcnv$name[gcnv$PASS_QS])
pass_freq <- paste0(gcnv$sample[gcnv$PASS_freq], gcnv$name[gcnv$PASS_freq])
pass_samp <- paste0(gcnv$sample[gcnv$PASS_sample], gcnv$name[gcnv$PASS_sample])

library(RColorBrewer)
myCol <- brewer.pal(3, "Pastel2")
myCol <- wes_palette("Darjeeling1")[1:3]
library(VennDiagram)
venn.diagram(
  x = list(pass_qs, pass_freq, pass_samp),
  category.names = c("pass_qs" , "pass_freq" , "pass_sample"),
  filename = paste0(wd, 'venn_diagramm_a.png'),
  output=TRUE,
  print.mode = c("raw", "percent"),
  
  # Output features
  imagetype="png" ,
  height = 900 , 
  width = 900 , 
  resolution = 300,
  compression = "lzw",
  
  # Circles
  lwd = 2,
  lty = 'blank',
  fill = myCol,
  
  # Numbers
  cex = .6,
  fontface = "bold",
  fontfamily = "sans",
  
  # Set names
  cat.cex = 0.6,
  cat.fontface = "bold",
  cat.default.pos = "outer",
  cat.pos = c(-27, 27, 135),
  cat.dist = c(0.055, 0.055, 0.085),
  cat.fontfamily = "sans",
  rotation = 1
)
```


